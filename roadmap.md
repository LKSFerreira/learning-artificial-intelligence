# üó∫Ô∏è Roadmap Completo de Aprendizado de IA

Este roadmap detalha **passo a passo** todas as etapas necess√°rias para dominar Intelig√™ncia Artificial, desde o setup at√© projetos avan√ßados. Cada fase cont√©m explica√ß√µes did√°ticas do **porqu√™** de cada tarefa.

---

## üìã Fase 0: Setup Profissional

**Objetivo:** Configurar um ambiente de desenvolvimento Python profissional com tooling moderno, seguindo boas pr√°ticas da ind√∫stria.

**Por que esta fase importa:** Um projeto bem estruturado facilita manuten√ß√£o, debug e escalabilidade. Aprender isso desde o in√≠cio cria h√°bitos profissionais.

### 0.1 Limpeza do Reposit√≥rio

**Explica√ß√£o:** Estamos iniciando uma jornada nova. C√≥digo antigo pode confundir e criar conflitos. Vamos arquiv√°-lo corretamente.

- [ ] Criar branch `legacy` para preservar c√≥digo antigo
  - Por qu√™: Git permite salvar hist√≥rico sem poluir branch principal
- [ ] Deletar pastas `ai-game-learning/` e `personal-portfolio/`
  - Por qu√™: Manter projeto focado e organizado
- [ ] Fazer commit limpo no `main`
  - Por qu√™: Marco zero do projeto reestruturado

### 0.2 Estrutura de Pastas

**Explica√ß√£o:** Python moderno usa estrutura espec√≠fica para facilitar imports, testes e distribui√ß√£o.

- [ ] Criar `src/ai_game_learning/` (pacote principal)
  - Por qu√™: Separar c√≥digo-fonte de testes e docs
  - Como: `mkdir -p src/ai_game_learning && touch src/ai_game_learning/__init__.py`
- [ ] Criar `tests/` (testes unit√°rios)
  - Por qu√™: Testar c√≥digo garante que funciona conforme esperado
  - Como: `mkdir tests && touch tests/__init__.py`
- [ ] Criar `docs/` (documenta√ß√£o das fases)
  - Por qu√™: Documentar aprendizado e decis√µes t√©cnicas
  - Como: `mkdir -p docs`
- [ ] Criar `notebooks/` (experimentos Jupyter)
  - Por qu√™: Jupyter √© ideal para visualiza√ß√µes e experimentos interativos
  - Como: `mkdir notebooks`

### 0.3 Tooling Python Moderno

**Explica√ß√£o:** Ferramentas profissionais automatizam qualidade de c√≥digo: formata√ß√£o consistente, detec√ß√£o de erros de tipo, e testes automatizados.

- [ ] Criar `pyproject.toml` com metadados do projeto
  - Por qu√™: Padr√£o moderno Python para configura√ß√£o centralizada
  - Como: Criar arquivo com nome, vers√£o, depend√™ncias
- [ ] Configurar **Ruff** (linter + formatter ultra r√°pido)
  - Por qu√™: Detecta erros de estilo e boas pr√°ticas automaticamente
  - Como: `pip install ruff` e configurar em `pyproject.toml`
- [ ] Configurar **MyPy** (type checking)
  - Por qu√™: Detecta erros de tipo antes de executar c√≥digo
  - Como: `pip install mypy` e configurar em `pyproject.toml`
- [ ] Configurar **Pytest** (framework de testes)
  - Por qu√™: Automatiza testes e garante c√≥digo funcional
  - Como: `pip install pytest` e adicionar a `requirements-dev.txt`
- [ ] Criar `.venv` e instalar depend√™ncias
  - Por qu√™: Ambiente virtual isola projeto de instala√ß√µes globais
  - Como: `.venv/Scripts/python.exe -m venv .venv`

### 0.4 Verifica√ß√£o

**Explica√ß√£o:** Testar que tooling est√° funcionando antes de come√ßar.

- [ ] Rodar `ruff check .` sem erros
- [ ] Rodar `ruff format .` para formatar c√≥digo
- [ ] Rodar `mypy src/` sem erros de tipo

---

## üéì Fase 1: Fundamentos Te√≥ricos

**Objetivo:** Construir base s√≥lida de conhecimento em IA, entendendo conceitos antes de codificar.

**Por que esta fase importa:** C√≥digo sem entendimento √© decoreba. Entender a teoria permite adaptar solu√ß√µes a qualquer problema.

### 1.1 Conceitos Base

**Explica√ß√£o:** IA √© um campo amplo. Precisamos entender a hierarquia de conceitos.

- [ ] Estudar: O que √© Intelig√™ncia Artificial?
  - Por qu√™: Definir escopo do que vamos aprender
  - Recursos: ler `docs/fase-1.md` completo
- [ ] Estudar: Diferen√ßa entre IA, ML, DL
  - Por qu√™: Evitar confus√£o entre termos essenciais
  - Analogia: IA (oficina) > ML (ferramentas el√©tricas) > DL (impressora 3D)
- [ ] Documentar em `docs/fase_1_fundamentos.md`
  - Por qu√™: Escrever consolida aprendizado
  - Como: Resumir com pr√≥prias palavras

### 1.2 Reinforcement Learning (Teoria)

**Explica√ß√£o:** RL √© a t√©cnica que usaremos do in√≠cio ao fim. √â como treinar um cachorro com recompensas.

- [ ] Estudar os 5 componentes: Agente, Ambiente, Estado, A√ß√£o, Recompensa
  - Por qu√™: Todo sistema RL precisa destes elementos
  - Exemplo: Jogo da Velha - Agente (IA), Ambiente (tabuleiro), Estado (posi√ß√µes X/O), A√ß√£o (onde jogar), Recompensa (+1 vit√≥ria/-1 derrota)
- [ ] Entender o ciclo de intera√ß√£o Agente ‚Üî Ambiente
  - Por qu√™: RL √© um loop cont√≠nuo de decis√£o-a√ß√£o-feedback
  - Fluxo: Observar Estado ‚Üí Escolher A√ß√£o ‚Üí Receber Recompensa ‚Üí Atualizar Conhecimento ‚Üí Repetir
- [ ] Estudar: O que √© uma Pol√≠tica (Policy)?
  - Por qu√™: Pol√≠tica √© o "c√©rebro finalizado" da IA
  - Analogia: Manual de estrat√©gias que diz qual a√ß√£o tomar em cada situa√ß√£o

### 1.3 Matem√°tica Essencial

**Explica√ß√£o:** RL usa matem√°tica, mas vamos focar na **intui√ß√£o** n√£o em decorar f√≥rmulas.

- [ ] Estudar: Equa√ß√£o de Bellman (intui√ß√£o)
  - Por qu√™: √â a f√≥rmula que ensina a IA a aprender
  - Intui√ß√£o: "Valor de uma a√ß√£o = Recompensa agora + Valor esperado do futuro"
- [ ] Estudar: O que √© Valor Q (Quality)?
  - Por qu√™: Q-Table √© nosso primeiro algoritmo
  - Intui√ß√£o: "Qu√£o boa √© esta a√ß√£o nesta situa√ß√£o?"
- [ ] Estudar: Explora√ß√£o vs Exploitation (Epsilon-Greedy)
  - Por qu√™: IA precisa equilibrar testar coisas novas vs usar o que j√° sabe
  - Analogia: Explorar mapa novo vs farmar no spot conhecido

### 1.4 Recursos Recomendados

- [ ] Assistir: David Silver RL Lecture 1-2 (YouTube)
  - Por qu√™: Melhor curso acad√™mico de RL gratuito
- [ ] Ler: Sutton & Barto Cap√≠tulo 1-3
  - Por qu√™: B√≠blia do Reinforcement Learning

---

## üéÆ Fase 2: Q-Learning B√°sico (Jogo da Velha)

**Objetivo:** Implementar nosso primeiro agente de IA que aprende a jogar Jogo da Velha atrav√©s de Q-Learning.

**Por que esta fase importa:** Jogo da Velha √© simples o suficiente para entender cada linha de c√≥digo, mas complexo o suficiente para aprender RL de verdade.

### 2.1 Ambiente do Jogo

**Explica√ß√£o:** Antes da IA, criamos o "mundo" onde ela vai viver. O ambiente define as regras do jogo.

**Task 1: Criar estrutura do `ambiente.py`**

Por qu√™: Precisamos representar o tabuleiro em c√≥digo

- [ ] Criar arquivo `src/ai_game_learning/jogo_da_velha/ambiente.py`
- [ ] Criar classe `AmbienteJogoDaVelha`
- [ ] Implementar `__init__()` que cria tabuleiro vazio (lista de 9 espa√ßos)
  - Por qu√™: Lista indexada √© mais simples que matriz 3x3
  - Estrutura: `[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ']`

**Task 2: Implementar verifica√ß√£o de vit√≥ria**

Por qu√™: Precisamos saber quando o jogo terminou

- [ ] Criar m√©todo `verificar_vitoria(simbolo: str) -> bool`
- [ ] Implementar checagem de 3 linhas horizontais
  - Posi√ß√µes: [0,1,2], [3,4,5], [6,7,8]
- [ ] Implementar checagem de 3 colunas verticais
  - Posi√ß√µes: [0,3,6], [1,4,7], [2,5,8]
- [ ] Implementar checagem de 2 diagonais
  - Posi√ß√µes: [0,4,8], [2,4,6]

**Task 3: Implementar lista de a√ß√µes v√°lidas**

Por qu√™: IA s√≥ pode jogar em posi√ß√µes vazias

- [ ] Criar m√©todo `obter_acoes_validas() -> list[int]`
- [ ] Retornar lista de √≠ndices onde `tabuleiro[i] == ' '`
- [ ] Escrever teste para verificar funcionamento

**Task 4: Testes unit√°rios**

Por qu√™: Garantir que ambiente funciona antes de criar IA

- [ ] Criar `tests/test_ambiente_jogo_velha.py`
- [ ] Testar: tabuleiro inicia vazio
- [ ] Testar: detecta vit√≥ria nas 8 combina√ß√µes poss√≠veis
- [ ] Testar: retorna a√ß√µes v√°lidas corretamente

### 2.2 Agente Q-Learning

**Explica√ß√£o:** Agora criamos o "c√©rebro" da IA. Ele usa uma tabela (Q-Table) para armazenar o que aprendeu.

**Task 1: Estrutura b√°sica do `agente.py`**

Por qu√™: Organizar c√≥digo da IA separadamente do ambiente

- [ ] Criar arquivo `src/ai_game_learning/jogo_da_velha/agente.py`
- [ ] Criar classe `AgenteQLearning`
- [ ] Implementar `__init__(alpha: float, gamma: float, epsilon: float)`
  - `alpha`: taxa de aprendizado (0.1 padr√£o)
  - `gamma`: fator de desconto (0.9 padr√£o)
  - `epsilon`: taxa de explora√ß√£o (1.0 no in√≠cio)
- [ ] Inicializar `self.q_table: dict = {}` (dicion√°rio vazio)

**Task 2: Implementar Q-Table**

Por qu√™: Q-Table √© a "mem√≥ria" da IA

- [ ] Criar m√©todo `obter_valor_q(estado: str, acao: int) -> float`
- [ ] Se `(estado, acao)` n√£o existe na tabela, retornar `0.0`
- [ ] Caso contr√°rio, retornar valor armazenado
- [ ] Criar m√©todo `definir_valor_q(estado: str, acao: int, valor: float)`
  - Armazena/atualiza valor Q na tabela

**Task 3: Implementar escolha aleat√≥ria**

Por qu√™: IA precisa explorar no in√≠cio do treino

- [ ] Criar m√©todo `escolher_acao_aleatoria(acoes_validas: list[int]) -> int`
- [ ] Usar `random.choice(acoes_validas)`
- [ ] Importar `import random` no topo do arquivo

### 2.3 Estrat√©gia Epsilon-Greedy

**Explica√ß√£o:** Balancear explora√ß√£o (tentar coisas novas) vs exploitation (usar melhor conhecida).

**Task 1: Implementar escolha gulosa (greedy)**

Por qu√™: Escolher melhor a√ß√£o conhecida

- [ ] Criar m√©todo `escolher_melhor_acao(estado: str, acoes_validas: list[int]) -> int`
- [ ] Para cada a√ß√£o v√°lida, obter `obter_valor_q(estado, acao)`
- [ ] Retornar a√ß√£o com maior valor Q
- [ ] Se empate, escolher aleatoriamente entre empatadas

**Task 2: Implementar epsilon-greedy**

Por qu√™: Combinar explora√ß√£o + exploitation

- [ ] Criar m√©todo `escolher_acao(estado: str, acoes_validas: list[int]) -> int`
- [ ] Gerar n√∫mero aleat√≥rio `r = random.random()` (entre 0 e 1)
- [ ] Se `r < epsilon`: explorar (escolher aleat√≥ria)
- [ ] Sen√£o: exploitar (escolher melhor conhecida)

**Task 3: Testar estrat√©gia**

- [ ] Testar com `epsilon=1.0` - deve escolher sempre aleat√≥rio
- [ ] Testar com `epsilon=0.0` - deve escolher sempre melhor
- [ ] Testar com `epsilon=0.5` - deve misturar 50/50

### 2.4 Equa√ß√£o de Bellman

**Explica√ß√£o:** A f√≥rmula que atualiza a Q-Table ap√≥s cada jogada. Nova Opini√£o = Opini√£o Antiga + Œ± √ó Surpresa

**Task 1: Implementar atualiza√ß√£o Q**

Por qu√™: √â como a IA aprende depois de cada a√ß√£o

- [ ] Criar m√©todo `atualizar_q(estado: str, acao: int, recompensa: float, proximo_estado: str, acoes_proximas: list[int])`
- [ ] Obter Q atual: `q_atual = obter_valor_q(estado, acao)`
- [ ] Calcular melhor Q futuro: `max_q_futuro = max([obter_valor_q(proximo_estado, a) for a in acoes_proximas])`
- [ ] Se jogo terminou: `max_q_futuro = 0` (n√£o h√° futuro)
- [ ] Calcular novo Q: `novo_q = q_atual + alpha * (recompensa + gamma * max_q_futuro - q_atual)`
- [ ] Armazenar: `definir_valor_q(estado, acao, novo_q)`

**Task 2: Experimento gamma = 0 (m√≠ope)**

Por qu√™: Entender impacto do fator de desconto

- [ ] Criar agente com `gamma=0`
- [ ] Treinar 1000 epis√≥dios
- [ ] Observar: IA √© "m√≠ope", s√≥ pensa no ganho imediato
- [ ] Documentar resultado

**Task 3: Experimento gamma = 1 (vision√°rio)**

- [ ] Criar agente com `gamma=1`
- [ ] Treinar 1000 epis√≥dios
- [ ] Observar: IA √© "vision√°ria", valoriza futuro igualmente ao presente
- [ ] Documentar resultado

**Task 4: Experimento alpha = 1.0**

Por qu√™: Entender taxa de aprendizado

- [ ] Criar agente com `alpha=1.0`
- [ ] Treinar 1000 epis√≥dios
- [ ] Observar: IA "esquece" tudo que sabia a cada nova experi√™ncia (muito inst√°vel)
- [ ] Documentar por que `alpha` moderado (0.1-0.3) √© melhor

### 2.5 Treinamento

**Explica√ß√£o:** Criar o "gin√°sio" onde a IA joga milhares de vezes para aprender.

**Task 1: Criar estrutura do treinador**

- [ ] Criar arquivo `src/ai_game_learning/jogo_da_velha/treinador.py`
- [ ] Criar fun√ß√£o `treinar(num_episodios: int, agente_x: AgenteQLearning, agente_o: AgenteQLearning)`

**Task 2: Implementar loop de epis√≥dios**

Por qu√™: Cada epis√≥dio √© uma partida completa

- [ ] Loop `for episodio in range(num_episodios):`
- [ ] Criar ambiente novo
- [ ] Alternar turnos entre X e O at√© jogo terminar
- [ ] Coletar recompensas no final

**Task 3: Implementar self-play**

Por qu√™: IA aprende jogando contra si mesma

- [ ] Ambos agentes usam a mesma Q-Table
- [ ] Cada jogada atualiza a tabela compartilhada
- [ ] Ao final: vencedor recebe `+1`, perdedor `-1`, empate `0`

**Task 4: Implementar decaimento de epsilon**

Por qu√™: Come√ßar explorando, terminar exploitando

- [ ] Iniciar com `epsilon = 1.0`
- [ ] A cada 100 epis√≥dios: `epsilon = epsilon * 0.99`
- [ ] M√≠nimo: `epsilon_min = 0.01`

### 2.6 Visualiza√ß√£o e An√°lise

**Explica√ß√£o:** Medir progresso visualmente.

- [ ] Criar `visualizador.py`
- [ ] Plotar taxa de vit√≥ria a cada 1000 epis√≥dios
- [ ] Usar `matplotlib` para gr√°ficos
- [ ] Salvar gr√°fico em `docs/fase_2_resultados.png`
- [ ] Documentar aprendizado em `docs/fase_2_resultados.md`

---

## üó∫Ô∏è Fase 3: Generaliza√ß√£o (Labirinto)

**Objetivo:** Provar que RL funciona para qualquer problema. Criar agente que navega labirintos.

**Por que esta fase importa:** Se RL funciona tanto para jogo de tabuleiro quanto navega√ß√£o espacial, funciona para qualquer coisa.

### 3.1 Novo Ambiente

**Explica√ß√£o:** Criar mundo em grade (Grid World) onde agente se move fisicamente.

**Task 1: Criar estrutura do ambiente**

- [ ] Criar `src/ai_game_learning/labirinto/ambiente_labirinto.py`
- [ ] Classe `AmbienteLabirinto`
- [ ] `__init__(tamanho: int, paredes: list[tuple], saida: tuple)`
- [ ] Representar como matriz `numpy.array(tamanho, tamanho)`
  - `0` = caminho livre, `1` = parede, `2` = sa√≠da

**Task 2: Implementar movimentos**

Por qu√™: Agente precisa se mover em 4 dire√ß√µes

- [ ] Criar m√©todo `mover(acao: str) -> tuple[float, bool]`
- [ ] A√ß√µes: `'cima', 'baixo', 'esquerda', 'direita'`
- [ ] Retornar: `(recompensa, terminou)`
- [ ] Validar: n√£o sair dos limites do grid

**Task 3: Implementar paredes e colis√£o**

- [ ] Se mover para parede: retornar `(-0.5, False)` e n√£o mover
- [ ] Se mover para espa√ßo livre: atualizar posi√ß√£o, retornar `(-0.1, False)`

**Task 4: Implementar sistema de recompensas**

Por qu√™: Ensinar IA a buscar caminho mais curto

- [ ] Chegar na sa√≠da: `(+10.0, True)`
- [ ] Bater na parede: `(-0.5, False)`
- [ ] Passo normal: `(-0.1, False)` (custo de energia)

### 3.2 Reutiliza√ß√£o do Agente

**Explica√ß√£o:** Ver se c√≥digo de Q-Learning funciona sem mudan√ßas.

**Task 1: Adaptar estados**

Por qu√™: Estado agora √© posi√ß√£o (x, y) n√£o configura√ß√£o de tabuleiro

- [ ] Criar m√©todo `estado_para_string(x: int, y: int) -> str`
- [ ] Retornar: `f"{x},{y}"`

**Task 2: Testar reutiliza√ß√£o**

- [ ] Usar mesma classe `AgenteQLearning` da Fase 2
- [ ] Verificar se funciona sem modifica√ß√µes
- [ ] Documentar: "Q-Learning √© gen√©rico!"

### 3.3 Experimentos

**Task 1: Labirinto 5x5 simples**

- [ ] Criar labirinto sem paredes
- [ ] Treinar 5000 epis√≥dios
- [ ] Verificar: IA aprende caminho reto para sa√≠da

**Task 2: Labirinto 10x10 com paredes**

- [ ] Criar labirinto com corredores
- [ ] Treinar 20000 epis√≥dios
- [ ] Visualizar: plotar Q-Table como setas no grid

**Task 3: Experimento: labirinto sem sa√≠da**

Por qu√™: Ver como IA lida com problema imposs√≠vel

- [ ] Criar labirinto onde sa√≠da √© cercada por paredes
- [ ] Treinar 10000 epis√≥dios
- [ ] Observar: todos Q-values convergem para negativos
- [ ] Documentar aprendizado

---

## üëÅÔ∏è Fase 4-7: Continua no Arquivo...

**Nota:** Fases 4-7 seguem mesma estrutura detalhada. Cada task tem:

- **Explica√ß√£o** do porqu√™
- **Passos granulares** acion√°veis
- **Checkpoints** de verifica√ß√£o

### Estrutura Resumida:

**Fase 4:** Vis√£o Computacional (OpenCV, captura de tela, detec√ß√£o de objetos, Dino do Chrome)

**Fase 5:** IA Reativa (Flappy Bird, l√≥gica baseada em regras vs RL)

**Fase 6:** Deep Q-Network (PyTorch, redes neurais, Experience Replay, CartPole/LunarLander)

**Fase 7+:** Projetos Avan√ßados Generalizados:

- Op√ß√£o A: MMORPGs (Ragnarok, WoW, etc.)
- Op√ß√£o B: Jogos de Estrat√©gia (StarCraft, Dota)
- Op√ß√£o C: Jogos de Plataforma (Super Mario, Celeste)
- Op√ß√£o D: Simuladores (Self-Driving Car, Drone Control)

---

## üìä Acompanhamento de Progresso

Marque `[x]` conforme completa tasks. Estrutura permite sentir progresso incremental.

**Checkpoint Fase 0:** ‚úÖ Ambiente configurado profissionalmente
**Checkpoint Fase 1:** ‚úÖ Fundamentos te√≥ricos consolidados
**Checkpoint Fase 2:** ‚úÖ Primeiro agente RL funcionando
**Checkpoint Fase 3:** ‚úÖ Conceitos generalizados comprovados
**Checkpoint Fase 4:** ‚úÖ IA enxergando o mundo
**Checkpoint Fase 5:** ‚úÖ IA jogando em tempo real
**Checkpoint Fase 6:** ‚úÖ Deep Learning dominado
**Checkpoint Fase 7+:** ‚úÖ Projetos complexos do mundo real

---

## üéØ Filosofia do Roadmap

**Granularidade:** Tasks pequenas e acion√°veis (15-30 min cada)
**Did√°tica:** Cada task explica o "porqu√™", n√£o s√≥ o "como"
**Progress√£o:** Cada fase prepara para pr√≥xima naturalmente
**Generaliza√ß√£o:** Exemplos diversos, n√£o focados em um √∫nico jogo
**Build to Break:** Incentiva experimentos e quebrar c√≥digo para aprender
