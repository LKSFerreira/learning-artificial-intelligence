# üß† Fase 3: O agente no Labirinto ‚Äî Navega√ß√£o em Grid World

Bem-vindo √† Fase 3! Se na fase anterior nossa IA aprendeu a jogar um jogo de tabuleiro est√°tico, agora ela vai aprender a **andar**.

### O que vamos construir?

Uma Intelig√™ncia Artificial que aprende a navegar em um **Labirinto** (Grid World) para encontrar a sa√≠da o mais r√°pido poss√≠vel, evitando paredes e becos sem sa√≠da.

1.  **O Agente:** Um "agente virtual" que pode se mover para Cima, Baixo, Esquerda e Direita.
2.  **O Ambiente:** Um labirinto 10x10 com paredes e uma sa√≠da.
3.  **O Objetivo:** Encontrar o caminho mais curto at√© a sa√≠da.

---

## üó∫Ô∏è O Mundo em Grade (Grid World)

Este √© o "Hello World" da navega√ß√£o em rob√≥tica e jogos. Diferente do Jogo da Velha, onde o estado era a configura√ß√£o das pe√ßas, aqui o estado √© **onde voc√™/agente est√°**.

### 1. O Estado: "Onde estou?"

No Jogo da Velha, o estado era complexo (uma `tupla` de 9 itens). Aqui, √© muito mais simples e visual.
O estado √© apenas a **coordenada (Linha, Coluna)** do nosso agente.

```
  0 1 2 3
0 . . . .
1 . A . .  <-- O Agente (A) est√° na linha 1, coluna 1.
2 . . . .      Estado = (1, 1)
3 . . S .
```

Tupla:  √â como uma "lista" de itens que n√£o pode ser alterada depois de criada. Pense nela como um pacote fechado de informa√ß√µes. Por exemplo, a coordenada `(1, 1)` √© uma tupla que agrupa a linha `1` e a coluna `1` em uma √∫nica unidade. Uma vez que voc√™ define `(1, 1)`, voc√™ n√£o pode mudar o `1` da linha para `2` dentro da mesma tupla; voc√™ teria que criar uma nova tupla `(2, 1)`.

### 2. As A√ß√µes: "Para onde vou?"

Em vez de "marcar X na posi√ß√£o 5", nossas a√ß√µes agora s√£o movimentos f√≠sicos:

- ‚¨ÜÔ∏è **Cima** (Up)
- ‚¨áÔ∏è **Baixo** (Down)
- ‚¨ÖÔ∏è **Esquerda** (Left)
- ‚û°Ô∏è **Direita** (Right)

### 3. A Recompensa: O Segredo da Pregui√ßa Inteligente ü¶•

Aqui entra um conceito crucial do Aprendizado por Refor√ßo: **Recompensas Negativas (Penalidades)**.

Imagine que voc√™ quer ensinar seu cachorro a vir at√© voc√™.

- Se ele chegar em voc√™: **Ganha um biscoito (+10)**.
- Mas se ele ficar dando voltas no jardim antes de vir: **N√£o acontece nada (0)**.

O cachorro pode aprender a dar 50 voltas no jardim e s√≥ depois ir at√© voc√™, porque o resultado final √© o mesmo: biscoito. Ele n√£o tem pressa.

Para a nossa IA, queremos o **caminho mais curto**. Como ensinamos pressa? **Cobrando "energia" por cada passo.**

- **Chegar na Sa√≠da:** +10.0 (O Biscoito üç™)
- **Bater na Parede:** -0.5 (A Dor de Cabe√ßa ü§ï)
- **Dar um Passo:** -0.1 (O Cansa√ßo üòÆ‚Äçüí®)

> **A L√≥gica da IA:** "Cada passo que eu dou me custa 0.1 pontos. Se eu demorar 100 passos, vou perder 10 pontos! Eu preciso correr para a sa√≠da para parar de perder pontos e ganhar logo meu biscoito."

Isso for√ßa a IA a encontrar o **caminho √≥timo**.

---

## üß† A Q-Table do Labirinto

Nossa "cola" (Q-Table) agora vai mapear cada **quadrado do ch√£o** para as 4 dire√ß√µes poss√≠veis.

Imagine que o ch√£o do labirinto tem setas desenhadas, indicando qual a melhor dire√ß√£o a seguir naquele quadrado. A Q-Table √© quem desenha essas setas.

**Exemplo de uma linha da Q-Table para o Estado (1, 1):**

| Estado (Posi√ß√£o) | Cima ‚¨ÜÔ∏è | Baixo ‚¨áÔ∏è | Esquerda ‚¨ÖÔ∏è | Direita ‚û°Ô∏è |
| :--------------- | :-----: | :------: | :---------: | :--------: |
| **(1, 1)**       |  -0.5   |   -0.1   |    -0.5     |  **0.8**   |

- **Cima (-0.5):** Parede! Ruim.
- **Esquerda (-0.5):** Parede! Ruim.
- **Baixo (-0.1):** Caminho livre, mas leva para longe da sa√≠da.
- **Direita (0.8):** Caminho livre e aproxima da sa√≠da! **Melhor A√ß√£o.**

---

## üéÆ Do Tabuleiro para o Mapa

A grande mudan√ßa mental desta fase √© perceber que **RL serve para qualquer coisa**.

- No Jogo da Velha, "naveg√°vamos" por estados de um tabuleiro.
- No Labirinto, navegamos por posi√ß√µes f√≠sicas.
- No Ragnarok (futuro), navegaremos por mapas reais do jogo.

A matem√°tica (Q-Learning, Bellman) √© **exatamente a mesma**. S√≥ mudamos o que chamamos de "Estado" e "A√ß√£o".

## üöÄ O que vamos construir?

| Arquivo        | Responsabilidade                                                          |
| :------------- | :------------------------------------------------------------------------ |
| `ambiente.py`  | (J√° existe) Cria o labirinto, desenha paredes e define as regras.         |
| `agente.py`    | (Sua Miss√£o) O c√©rebro que decide para onde andar.                        |
| `treinador.py` | O "gym" onde o agente vai correr milhares de vezes at√© decorar o caminho. |
| `jogar.py`     | (J√° existe) Para vermos o resultado visualmente.                          |

Preparado para guiar nosso rato virtual? Vamos para o c√≥digo!
