# ğŸ§  AI Game Learning â€” Do Zero Ã  IA Jogadora

## ğŸ¯ Objetivo Final

Aprender **InteligÃªncia Artificial** do zero absoluto atÃ© criar uma IA capaz de **jogar Ragnarok Online Brasil (bRO)**.

---

## âš ï¸ Metodologia: Build to Break

| âœ… Fazer                                 | âŒ Evitar                      |
| ---------------------------------------- | ------------------------------ |
| Implementar eu mesmo antes de consultar  | Pedir cÃ³digo completo para LLM |
| Usar LLM para explicar conceitos/erros   | Aceitar cÃ³digo sem entender    |
| Quebrar o cÃ³digo para ver o que acontece | Seguir tutoriais cegamente     |

---

## ğŸ§­ Roadmap Completo

### Fase 0: Setup Profissional â¬œ

#### 0.1 Limpeza do RepositÃ³rio

- [ ] Criar branch `legacy` com cÃ³digo antigo
- [ ] Deletar pastas `ai-game-learning/` e `personal-portfolio/`
- [ ] Fazer commit limpo no `main`

#### 0.2 Estrutura de Pastas

- [ ] Criar `src/ai_game_learning/` (pacote principal)
- [ ] Criar `tests/` (testes unitÃ¡rios)
- [ ] Criar `docs/` (documentaÃ§Ã£o das fases)
- [ ] Criar `notebooks/` (experimentos Jupyter)

#### 0.3 Tooling Python Moderno

- [ ] Criar `pyproject.toml` com metadados do projeto
- [ ] Configurar **Ruff** (linter + formatter)
- [ ] Configurar **MyPy** (type checking)
- [ ] Configurar **Pytest** (testes)
- [ ] Criar `.venv` e instalar dependÃªncias

#### 0.4 VerificaÃ§Ã£o

- [ ] Rodar `ruff check .` sem erros
- [ ] Rodar `ruff format .`
- [ ] Rodar `mypy src/` sem erros

---

### Fase 1: Fundamentos TeÃ³ricos â¬œ

#### 1.1 Conceitos Base

- [ ] Estudar: O que Ã© InteligÃªncia Artificial?
- [ ] Estudar: DiferenÃ§a entre IA, ML, DL
- [ ] Documentar em `docs/fase_1_fundamentos.md`

#### 1.2 Reinforcement Learning (Teoria)

- [ ] Estudar os 5 componentes: Agente, Ambiente, Estado, AÃ§Ã£o, Recompensa
- [ ] Entender o ciclo de interaÃ§Ã£o Agente â†” Ambiente
- [ ] Estudar: O que Ã© uma PolÃ­tica (Policy)?

#### 1.3 MatemÃ¡tica Essencial

- [ ] Estudar: EquaÃ§Ã£o de Bellman (intuiÃ§Ã£o, nÃ£o decorar fÃ³rmula)
- [ ] Estudar: O que Ã© Valor Q (Quality)?
- [ ] Estudar: ExploraÃ§Ã£o vs ExploraÃ§Ã£o (Epsilon-Greedy)

#### 1.4 Recursos Recomendados

- [ ] Assistir: David Silver RL Lecture 1-2
- [ ] Ler: Sutton & Barto CapÃ­tulo 1-3

---

### Fase 2: Q-Learning BÃ¡sico (Jogo da Velha) â¬œ

#### 2.1 Ambiente do Jogo

- [ ] Criar `ambiente.py` do zero
- [ ] Implementar representaÃ§Ã£o do tabuleiro (lista/array)
- [ ] Implementar verificaÃ§Ã£o de vitÃ³ria
- [ ] Implementar lista de aÃ§Ãµes vÃ¡lidas
- [ ] Escrever testes para o ambiente

#### 2.2 Agente Q-Learning

- [ ] Criar `agente.py` do zero
- [ ] Implementar Q-Table (dicionÃ¡rio)
- [ ] Implementar `obter_valor_q(estado, acao)`
- [ ] Implementar escolha aleatÃ³ria (exploraÃ§Ã£o)

#### 2.3 EstratÃ©gia Epsilon-Greedy

- [ ] Implementar `escolher_acao(estado, epsilon)`
- [ ] Testar: epsilon=1.0 (100% aleatÃ³rio)
- [ ] Testar: epsilon=0.0 (100% guloso)

#### 2.4 EquaÃ§Ã£o de Bellman

- [ ] Implementar `atualizar_q(estado, acao, recompensa, proximo_estado)`
- [ ] **Experimento**: `gamma = 0` (mÃ­ope) - O que acontece?
- [ ] **Experimento**: `gamma = 1` (visionÃ¡rio) - O que muda?
- [ ] **Experimento**: `alpha = 1.0` - Por que Ã© ruim?

#### 2.5 Treinamento

- [ ] Criar `treinador.py`
- [ ] Implementar loop de episÃ³dios
- [ ] Implementar self-play (agente vs agente)
- [ ] Implementar decaimento de epsilon

#### 2.6 VisualizaÃ§Ã£o e AnÃ¡lise

- [ ] Plotar evoluÃ§Ã£o da taxa de vitÃ³ria
- [ ] Visualizar Q-Table para estados especÃ­ficos
- [ ] Documentar resultados em `docs/fase_2_resultados.md`

---

### Fase 3: GeneralizaÃ§Ã£o (Labirinto) â¬œ

#### 3.1 Novo Ambiente

- [ ] Criar ambiente de grade (Grid World)
- [ ] Implementar movimentos: cima, baixo, esquerda, direita
- [ ] Implementar paredes e objetivo
- [ ] Implementar sistema de recompensas (-0.1 por passo, +10 objetivo)

#### 3.2 ReutilizaÃ§Ã£o do Agente

- [ ] Adaptar agente Q-Learning para o labirinto
- [ ] Verificar: O algoritmo funciona sem mudanÃ§as?
- [ ] Se nÃ£o, entender o porquÃª

#### 3.3 Experimentos

- [ ] Testar com labirinto 5x5
- [ ] Testar com labirinto 10x10
- [ ] **Experimento**: Labirinto sem saÃ­da - O que acontece?

---

### Fase 4: VisÃ£o Computacional BÃ¡sica â¬œ

#### 4.1 OpenCV Fundamentos

- [ ] Instalar e configurar OpenCV
- [ ] Capturar screenshot da tela
- [ ] Converter para escala de cinza
- [ ] Detectar bordas (Canny)

#### 4.2 DetecÃ§Ã£o de Objetos Simples

- [ ] Detectar retÃ¢ngulos/formas
- [ ] Template matching (encontrar imagem dentro de imagem)
- [ ] Detectar cores especÃ­ficas (HSV)

#### 4.3 Projeto: Dino do Chrome

- [ ] Capturar tela do jogo
- [ ] Detectar cactos (obstÃ¡culos)
- [ ] Implementar lÃ³gica: "Se cacto prÃ³ximo â†’ pular"
- [ ] Integrar com PyAutoGUI para controle

---

### Fase 5: IA Reativa (Flappy Bird) â¬œ

#### 5.1 Ambiente

- [ ] Encontrar/criar clone de Flappy Bird jogÃ¡vel
- [ ] Capturar tela e identificar elementos
- [ ] Detectar: posiÃ§Ã£o do pÃ¡ssaro, posiÃ§Ã£o dos canos

#### 5.2 Agente Reativo

- [ ] Implementar lÃ³gica baseada em regras
- [ ] Testar diferentes heurÃ­sticas
- [ ] Documentar qual funciona melhor

#### 5.3 Agente Aprendiz (Opcional)

- [ ] Aplicar Q-Learning ao Flappy Bird
- [ ] Comparar com agente reativo

---

### Fase 6: Deep Q-Network (DQN) â¬œ

#### 6.1 PyTorch Fundamentos

- [ ] Instalar PyTorch
- [ ] Criar tensores e operaÃ§Ãµes bÃ¡sicas
- [ ] Entender autograd (gradientes automÃ¡ticos)

#### 6.2 Rede Neural Simples

- [ ] Criar rede com 1 camada oculta
- [ ] Treinar para funÃ§Ã£o XOR (sanity check)
- [ ] Entender forward pass e backpropagation

#### 6.3 DQN Teoria

- [ ] Estudar: Por que substituir Q-Table por rede neural?
- [ ] Estudar: Experience Replay
- [ ] Estudar: Target Network

#### 6.4 Gymnasium

- [ ] Instalar Gymnasium
- [ ] Explorar ambiente CartPole
- [ ] Explorar ambiente LunarLander

#### 6.5 Implementar DQN

- [ ] Criar rede neural para aproximar Q
- [ ] Implementar Experience Replay
- [ ] Treinar em CartPole
- [ ] Treinar em LunarLander

---

### Fase 7: Ragnarok Online (Projeto Final) â¬œ

#### 7.1 AnÃ¡lise do Jogo

- [ ] Identificar elementos visuais (HP, SP, monstros, itens)
- [ ] Mapear teclas de aÃ§Ã£o (F1-F9, cliques)
- [ ] Definir estados possÃ­veis do agente

#### 7.2 Captura e Processamento

- [ ] Capturar tela do jogo em tempo real
- [ ] Detectar barra de HP/SP
- [ ] Detectar monstros na tela
- [ ] Detectar itens dropados

#### 7.3 MÃ¡quina de Estados

- [ ] Implementar estados: IDLE, ATACANDO, CURANDO, FUGINDO, COLETANDO
- [ ] Definir transiÃ§Ãµes entre estados
- [ ] Integrar com visÃ£o computacional

#### 7.4 Controle

- [ ] Integrar PyAutoGUI ou AutoHotkey
- [ ] Implementar movimentaÃ§Ã£o
- [ ] Implementar uso de skills
- [ ] Implementar coleta de itens

#### 7.5 Agente Inteligente

- [ ] Aplicar RL para otimizar comportamento
- [ ] Treinar em servidor privado/teste
- [ ] Documentar resultados

---

## ğŸ§° Tecnologias por Fase

| Fase | Tecnologias                                   |
| ---- | --------------------------------------------- |
| 0-3  | Python, NumPy, Matplotlib, Ruff, MyPy, Pytest |
| 4-5  | + OpenCV, PyAutoGUI                           |
| 6    | + PyTorch, Gymnasium                          |
| 7    | + AutoHotkey, VisÃ£o em tempo real             |

---

## ğŸ“– Recursos

- [Sutton & Barto - RL: An Introduction](http://incompleteideas.net/book/the-book.html)
- [David Silver - RL Course](https://www.youtube.com/playlist?list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ)
- [Spinning Up in Deep RL](https://spinningup.openai.com/)
- [PyTorch Tutorials](https://pytorch.org/tutorials/)

---

## ğŸ‘¤ Autor

**Lucas Ferreira (LKS)**

ğŸ“… InÃ­cio: 26/10/2025 | ğŸ“… RecomeÃ§o: 12/2025

> "A melhor forma de aprender IA Ã© quebrando a cabeÃ§a com ela."

---

## ğŸ“œ LicenÃ§a

MIT
