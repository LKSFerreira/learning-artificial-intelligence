import { Phase } from './types';

export const CURRICULUM: Phase[] = [
  {
    id: 1,
    title: "Fundamentos de IA",
    description: "Do zero aos conceitos de Redes Neurais e RL.",
    steps: [
      {
        id: "intro",
        title: "O que √© IA? ü§ñ",
        content: `Ol√°, futuro mestre de IAs! Bem-vindo ao ponto de partida da nossa jornada.

Antes de ensinarmos uma m√°quina a derrotar Porings e MVPs no Ragnarok, precisamos entender como ela "pensa".

**O que √© IA?**
Pense na IA como o grande sonho da computa√ß√£o: **a arte de criar m√°quinas que podem pensar, aprender e tomar decis√µes como seres humanos.**

√â o conceito geral que abrange desde a Siri no seu celular at√© os rob√¥s dos filmes de fic√ß√£o cient√≠fica.`,
        visualState: "intro_concept",
        type: 'content'
      },
      {
        id: "hierarchy",
        title: "A Caixa de Ferramentas üß∞",
        content: `Essa √© a parte que mais causa confus√£o, mas vamos simplificar com uma analogia.

Imagine que a **Intelig√™ncia Artificial (IA)** √© a sua oficina inteira.

*   **Machine Learning (ML)** √© o seu conjunto de **ferramentas el√©tricas** (furadeiras, serras). S√£o ferramentas poderosas que aprendem a fazer o trabalho sozinhas se voc√™ mostrar exemplos.
*   **Deep Learning (DL)** √© a ferramenta mais avan√ßada da sua oficina: uma **impressora 3D ou uma cortadora a laser**. √â uma vers√£o super especializada e poderosa do Machine Learning, inspirada no c√©rebro humano.

**Conclus√£o:** Todo Deep Learning √© Machine Learning, e todo Machine Learning √© Intelig√™ncia Artificial. Mas o contr√°rio n√£o √© verdadeiro.`,
        visualState: "hierarchy_toolbox",
        type: 'content'
      },
      {
        id: "ml_vs_trad",
        title: "ML: Aprendendo com Exemplos üì∏",
        content: `> Em vez de programar regras, n√≥s deixamos a m√°quina **aprender as regras sozinha** a partir de dados.

Imagine ensinar um computador a reconhecer uma **Po√ß√£o Vermelha**.

*   **Programa√ß√£o Tradicional:** Voc√™ escreveria regras r√≠gidas: \`SE pixel vermelho E formato vidro ENT√ÉO po√ß√£o\`. Fr√°gil.
*   **Machine Learning:** Voc√™ mostra **10.000 imagens** de po√ß√µes. O algoritmo descobre os padr√µes sozinho.

Vamos usar ML para que nossa IA aprenda o que √© um monstro ou um item apenas olhando a tela.`,
        visualState: "ml_examples",
        type: 'content'
      },
      {
        id: "deep_learning",
        title: "DL: A Linha de Montagem üè≠",
        content: `Deep Learning usa **Redes Neurais Artificiais**. Pense em uma linha de montagem:

1.  **Entrada:** A imagem do jogo.
2.  **Camada 1:** Detecta linhas e curvas.
3.  **Camada 2:** Monta formas (olhos, boca).
4.  **Camada 3:** Reconhece o "Poring".
5.  **Sa√≠da:** "98% de certeza que √© um Poring!".

O "Deep" vem das muitas camadas de processamento.`,
        visualState: "dl_neural_net",
        type: 'content'
      },
      {
        id: "rl_intro",
        title: "Aprendizado por Refor√ßo üêï",
        content: `Chegamos ao cora√ß√£o do projeto. Como ensinamos a IA a **agir**?

**Analogia: Adestrando um Cachorro üêï**

1.  **Comando:** "Senta!"
2.  **A√ß√£o:** O cachorro senta.
3.  **Feedback (Recompensa):** Voc√™ d√° um biscoito! ‚úÖ

Se ele latir em vez de sentar, n√£o ganha nada. ‚ùå

**RL √© isso:** aprendizado por tentativa e erro, guiado por recompensas. A IA descobre sozinha como ganhar mais "biscoitos" (pontos).`,
        visualState: "rl_dog_training",
        type: 'content'
      },
      {
        id: "rl_components",
        title: "Os 5 Pilares do RL üèõÔ∏è",
        content: `Todo sistema de RL tem 5 componentes fundamentais:

1.  **Agente:** O c√©rebro (nossa IA).
2.  **Ambiente:** O mundo (o jogo, o tabuleiro).
3.  **Estado:** A situa√ß√£o atual (foto da tela).
4.  **A√ß√£o:** O que o agente faz (atacar, andar).
5.  **Recompensa:** O feedback (+10 pontos, -100 de vida).`,
        visualState: "rl_components_interactive",
        type: 'content'
      },
      {
        id: "rl_cycle",
        title: "O Ciclo de Aprendizado üîÑ",
        content: `Esses componentes formam um loop infinito:

1.  Agente observa o **Estado**.
2.  Agente escolhe uma **A√ß√£o**.
3.  Ambiente muda e devolve uma **Recompensa**.
4.  Agente **Aprende**.
5.  Repete.

Treinar a IA √© rodar esse ciclo milh√µes de vezes.`,
        visualState: "rl_cycle_animation",
        type: 'content'
      },
      {
        id: "concepts_extra",
        title: "Pol√≠tica & Explora√ß√£o üß≠",
        content: `Dois conceitos finais:

**Pol√≠tica (Policy):**
√â o "manual" final da IA. *"Se vida baixa, usar po√ß√£o"*.

**Explora√ß√£o vs Exploitation:**
O dilema do aventureiro.
*   **Explorar:** Tentar algo novo (pode ser ruim, ou descobrir um tesouro).
*   **Exploitar:** Fazer o que j√° sabe que funciona (garante recompensa, mas n√£o evolui).

Um bom treinamento equilibra os dois.`,
        visualState: "exploration_exploitation",
        type: 'content'
      },
      {
        id: "video_lesson",
        title: "V√≠deo Aula: Fundamentos üçø",
        content: "Para consolidar tudo o que vimos, assista a esta aula r√°pida sobre os fundamentos.",
        visualState: "video_static",
        type: "video",
        videoUrl: "https://www.youtube.com/embed/7pi48LscJ2w"
      },
      {
        id: "quiz_phase1",
        title: "Desafio Final: Fundamentos üéì",
        content: "Voc√™ precisa acertar pelo menos **75%** das quest√µes para desbloquear a Fase 2.",
        visualState: "quiz_static",
        type: "quiz",
        quizData: [
          {
            id: "q1",
            question: "Qual a melhor defini√ß√£o para a hierarquia entre IA, Machine Learning e Deep Learning?",
            options: [
              "IA √© um subcampo do ML, que por sua vez √© um subcampo do Deep Learning.",
              "Deep Learning √© um subcampo do ML, que por sua vez √© um subcampo da IA.",
              "ML e Deep Learning s√£o abordagens distintas que comp√µem a √°rea da IA.",
              "S√£o tr√™s termos diferentes para descrever o mesmo conceito de automa√ß√£o."
            ],
            correctIndex: 1,
            explanation: "Lembre-se da Matrioska ou da Oficina: DL est√° dentro de ML, que est√° dentro de IA."
          },
          {
            id: "q2",
            question: "O que caracteriza fundamentalmente o Aprendizado por Refor√ßo (Reinforcement Learning)?",
            options: [
              "O aprendizado baseado em um grande volume de dados previamente rotulados.",
              "A descoberta de padr√µes e grupos em dados brutos sem supervis√£o humana.",
              "O aprendizado obtido pela intera√ß√£o com um ambiente, atrav√©s de recompensas.",
              "A constru√ß√£o de modelos que imitam a estrutura de redes neurais biol√≥gicas."
            ],
            correctIndex: 2,
            explanation: "RL √© sobre tentativa e erro guiado por feedback (recompensas/puni√ß√µes)."
          },
          {
            id: "q3",
            question: "No framework de RL, o componente respons√°vel por tomar decis√µes e executar a√ß√µes √© chamado de:",
            options: [
              "Ambiente (Environment), pois ele cont√©m todas as regras do mundo.",
              "Estado (State), pois ele representa a situa√ß√£o atual para a decis√£o.",
              "Agente (Agent), pois ele atua como o 'c√©rebro' do sistema de IA.",
              "Recompensa (Reward), pois ela guia o objetivo final das decis√µes."
            ],
            correctIndex: 2,
            explanation: "O Agente √© a entidade inteligente (nosso bot) que percebe o mundo e age sobre ele."
          },
          {
            id: "q4",
            question: "Para nossa IA do Ragnarok, o conjunto de informa√ß√µes como HP, SP, posi√ß√£o e monstros na tela representa o:",
            options: [
              "A√ß√£o (Action), que √© a jogada que o Agente pode executar.",
              "Estado (State), que √© a 'fotografia' atual do ambiente.",
              "Pol√≠tica (Policy), que √© a estrat√©gia geral aprendida pelo Agente.",
              "Ambiente (Environment), que √© o jogo como um todo."
            ],
            correctIndex: 1,
            explanation: "O Estado (State) cont√©m todas as vari√°veis necess√°rias para descrever o momento atual."
          },
          {
            id: "q5",
            question: "Quando um Agente executa uma a√ß√£o bem-sucedida e o ambiente lhe fornece um feedback positivo, ele recebe uma:",
            options: [
              "A√ß√£o (Action), que √© a escolha realizada pelo Agente.",
              "Pol√≠tica (Policy), que √© a estrat√©gia que ele est√° seguindo.",
              "Recompensa (Reward), que √© o sinal de feedback para o aprendizado.",
              "Estado (State), que √© a nova configura√ß√£o do ambiente."
            ],
            correctIndex: 2,
            explanation: "A Recompensa √© o 'biscoito' digital que diz ao agente se ele fez algo bom ou ruim."
          },
          {
            id: "q6",
            question: "O subcampo da IA focado em treinar Redes Neurais com m√∫ltiplas camadas para aprender padr√µes complexos √©:",
            options: [
              "Aprendizado por Refor√ßo, que foca no aprendizado por recompensa.",
              "Algoritmos Gen√©ticos, que simulam o processo de evolu√ß√£o natural.",
              "Sistemas Especialistas, baseados em um conjunto de regras l√≥gicas.",
              "Deep Learning, que utiliza arquiteturas de redes profundas."
            ],
            correctIndex: 3,
            explanation: "Deep Learning (Aprendizado Profundo) refere-se √† profundidade das camadas nas Redes Neurais."
          },
          {
            id: "q7",
            question: "No Jogo da Velha, o ato de o Agente escolher uma casa vazia para marcar um 'X' √© um exemplo de:",
            options: [
              "Um Estado, pois representa a configura√ß√£o do tabuleiro.",
              "Uma A√ß√£o, pois √© uma das jogadas poss√≠veis que o Agente pode fazer.",
              "Uma Recompensa, pois √© o feedback recebido ap√≥s a jogada.",
              "Uma Pol√≠tica, pois √© a estrat√©gia que guiou a escolha."
            ],
            correctIndex: 1,
            explanation: "A√ß√£o √© qualquer movimento ou interven√ß√£o que o agente faz no ambiente."
          },
          {
            id: "q8",
            question: "A estrat√©gia final, ou o 'manual de instru√ß√µes', que o Agente desenvolve ap√≥s um treinamento bem-sucedido √© chamada de:",
            options: [
              "Fun√ß√£o de Recompensa, o sistema que define os pontos.",
              "Modelo de Ambiente, a representa√ß√£o interna do jogo.",
              "Pol√≠tica (Policy), o mapeamento de estados para a√ß√µes.",
              "Taxa de Aprendizado, o par√¢metro que ajusta o treinamento."
            ],
            correctIndex: 2,
            explanation: "A Pol√≠tica define o comportamento do agente: dado um Estado X, execute a A√ß√£o Y."
          },
          {
            id: "q9",
            question: "O processo de 'treinar' uma IA de RL consiste em:",
            options: [
              "Fornecer um conjunto de dados com as respostas corretas para cada estado.",
              "Permitir que o Agente interaja com o ambiente repetidamente para otimizar suas a√ß√µes.",
              "Escrever manualmente as regras de decis√£o para todas as situa√ß√µes poss√≠veis.",
              "Compilar o c√≥digo-fonte do Agente em um formato execut√°vel pelo computador."
            ],
            correctIndex: 1,
            explanation: "O treino em RL √© pr√°tico: rodar milh√µes de ciclos de tentativa e erro."
          },
          {
            id: "q10",
            question: "O dilema do Agente entre usar uma estrat√©gia conhecida ou tentar uma nova para descobrir recompensas melhores √© chamado de:",
            options: [
              "O problema da Atribui√ß√£o de Cr√©dito, que define qual a√ß√£o gerou a recompensa.",
              "O desafio da Generaliza√ß√£o, que aplica o conhecimento a novas situa√ß√µes.",
              "O trade-off de Explora√ß√£o vs. 'Exploitation' (aproveitamento).",
              "A maldi√ß√£o da Dimensionalidade, relacionada √† complexidade do estado."
            ],
            correctIndex: 2,
            explanation: "Explora√ß√£o (arriscar o novo) vs Exploitation (garantir o certo) √© o dilema central do aprendizado."
          },
          {
            id: "q11",
            question: "Para a nossa IA, o jogo Ragnarok Online como um todo, com suas regras e mec√¢nicas, √© considerado o:",
            options: [
              "Agente (Agent), pois √© ele quem executa as l√≥gicas do jogo.",
              "Ambiente (Environment), pois √© o mundo com o qual o Agente interage.",
              "Estado (State), pois o jogo inteiro √© uma √∫nica situa√ß√£o.",
              "A√ß√£o (Action), pois o jogo representa uma a√ß√£o cont√≠nua."
            ],
            correctIndex: 1,
            explanation: "O Ambiente √© tudo aquilo que √© externo ao agente e onde ele opera."
          },
          {
            id: "q12",
            question: "Sistemas de recomenda√ß√£o, como os da Netflix ou Amazon, s√£o aplica√ß√µes cl√°ssicas de qual √°rea?",
            options: [
              "Aprendizado por Refor√ßo, pois aprendem com o feedback de acerto e erro.",
              "Processamento de Linguagem Natural, pois analisam textos de reviews.",
              "Vis√£o Computacional, pois identificam produtos em imagens.",
              "Machine Learning, pois aprendem padr√µes a partir do hist√≥rico do usu√°rio."
            ],
            correctIndex: 3,
            explanation: "Eles usam ML para prever prefer√™ncias baseadas em dados hist√≥ricos (Exemplos)."
          },
          {
            id: "q13",
            question: "Qual √© o objetivo principal que um Agente de Aprendizado por Refor√ßo tenta alcan√ßar?",
            options: [
              "Explorar o m√°ximo de estados diferentes dentro do ambiente, para conhec√™-lo.",
              "Maximizar a soma cumulativa de recompensas que recebe ao longo do tempo.",
              "Executar as a√ß√µes que foram programadas pelo desenvolvedor de forma eficiente.",
              "Encontrar o caminho mais curto para um estado terminal, independentemente das recompensas."
            ],
            correctIndex: 1,
            explanation: "O objetivo matem√°tico do RL √© sempre maximizar o retorno total (soma das recompensas)."
          },
          {
            id: "q14",
            question: "Dar uma recompensa de '-100' sempre que o personagem morre no jogo √© uma forma de:",
            options: [
              "Ensinar ao Agente um comportamento indesejado atrav√©s de uma recompensa negativa.",
              "Criar um bug no sistema, j√° que recompensas devem ser sempre positivas.",
              "Definir um estado inicial para o Agente, indicando o come√ßo do epis√≥dio.",
              "Aumentar a taxa de explora√ß√£o do Agente, for√ßando-o a tentar novas a√ß√µes."
            ],
            correctIndex: 0,
            explanation: "Puni√ß√µes (recompensas negativas) ensinam o agente o que *n√£o* fazer."
          },
          {
            id: "q15",
            question: "Por que Deep Learning √© uma abordagem eficaz para o reconhecimento de imagens em jogos?",
            options: [
              "Ele aprende a partir de regras l√≥gicas que descrevem o conte√∫do da imagem.",
              "Sua arquitetura em camadas √© ideal para aprender hierarquias de caracter√≠sticas visuais.",
              "Substitui a necessidade de uma GPU, podendo rodar em qualquer processador.",
              "√â um m√©todo que n√£o exige dados de treinamento para reconhecer novos objetos."
            ],
            correctIndex: 1,
            explanation: "As camadas convolucionais do DL s√£o excelentes para extrair formas, texturas e objetos de pixels brutos."
          }
        ]
      }
    ]
  },
  {
    id: 2,
    title: "Nosso Primeiro C√©rebro",
    description: "Criando uma IA para Jogo da Velha com Q-Learning.",
    steps: [
      {
        id: "intro_tictactoe",
        title: "O Desafio do Jogo da Velha ‚öîÔ∏è",
        content: `Seja bem-vindo ao nosso "dojo" de treinamento! √â aqui que a teoria da Fase 1 se transforma em c√≥digo e nossa primeira IA nasce.

**O que vamos construir?**
Uma Intelig√™ncia Artificial que aprende a jogar **Jogo da Velha** (Tic-Tac-Toe) do absoluto zero.

**A Jornada do Agente:**
1.  Come√ßa **ing√™nuo** (jogando aleatoriamente).
2.  Joga milhares de partidas contra si mesmo (**Self-Play**).
3.  Usa vit√≥rias, derrotas e empates para refinar sua estrat√©gia.
4.  Torna-se um **mestre invenc√≠vel**.

**Por que Jogo da Velha?**
√â o ambiente perfeito: simples, controlado e com aprendizado vis√≠vel. Se voc√™ dominar isso aqui, dominar√° o Ragnarok depois.`,
        visualState: "intro_concept",
        type: "content"
      },
      {
        id: "q_table_intro",
        title: "A M√°gica da Q-Table ‚ú®",
        content: `Vamos conhecer a principal ferramenta desta fase: a **Q-Table** (Tabela de Qualidade).

> **A ideia central do Q-Learning √© construir uma "cola" para o nosso Agente.**

Essa "cola" √© uma tabela gigante que mapeia **TODA situa√ß√£o poss√≠vel** do jogo para a qualidade de cada a√ß√£o.

**Exemplo A: Tabuleiro Vazio**
Imagine o in√≠cio do jogo. A IA olha para o tabuleiro vazio e consulta sua tabela.
Como ela ainda n√£o aprendeu nada, todos os valores s√£o **0.0**.
Conclus√£o: Ela n√£o tem prefer√™ncia e far√° um movimento aleat√≥rio.`,
        visualState: "q_table_zeros",
        type: "content"
      },
      {
        id: "critical_situation",
        title: "Situa√ß√µes Cr√≠ticas üõ°Ô∏è",
        content: `Aqui √© onde vemos a intelig√™ncia surgindo. Vamos analisar uma **Situa√ß√£o de Defesa Cr√≠tica** que a IA aprendeu ap√≥s milhares de jogos.

**O Cen√°rio:**
*   Voc√™ √© o **'O'**.
*   O advers√°rio **'X'** est√° prestes a ganhar na coluna da direita.
*   Voc√™ **N√ÉO** tem jogada de vit√≥ria imediata.

**O C√©rebro da IA (Q-Table):**
A IA analisou todas as jogadas poss√≠veis.
*   Jogadas normais: Levam √† derrota no pr√≥ximo turno (Recompensa futura ruim).
*   **Jogada de Bloqueio:** √â a √∫nica que evita a derrota imediata.

Na visualiza√ß√£o ao lado, veja como o valor **Q** da jogada de bloqueio √© muito superior √†s outras. Ela "sabe" que precisa bloquear para sobreviver.`,
        visualState: "critical_defense",
        type: "content"
      },
      {
        id: "bellman_analogy",
        title: "A Matem√°tica do XP üíé",
        content: `Como a IA calcula esses valores? Usamos a **Equa√ß√£o de Bellman**, mas vamos traduzi-la para "Gamer Speak".

Imagine que voc√™ √© o Agente ganhando XP:

> **Nova Opini√£o = Opini√£o Antiga + Taxa de Aprendizado √ó (Surpresa)**

Onde a **"Surpresa"** √©:
*(O que ganhei agora + O potencial futuro da minha jogada) - O que eu esperava.*

*   **Alpha (Œ±):** O quanto eu aprendo com cada experi√™ncia (Impulsivo vs Cauteloso).
*   **Gamma (Œ≥):** O quanto eu valorizo o futuro (Vision√°rio vs Imediatista).

√â assim que a IA atualiza sua mem√≥ria ap√≥s cada movimento!`,
        visualState: "bellman_equation",
        type: "content"
      },
      {
        id: "epsilon_greedy",
        title: "Explorar ou Farmar? üé≤",
        content: `Todo jogador enfrenta um dilema:
1.  **Exploitation (Farmar):** Fazer o que eu J√Å SEI que d√° certo (garante recompensa).
2.  **Exploration (Aventura):** Tentar algo novo e desconhecido (pode ser ruim, ou posso descobrir uma estrat√©gia melhor).

A estrat√©gia **Epsilon-Greedy** resolve isso:
A IA tem um "Medidor de Curiosidade" (**Epsilon**).
*   No come√ßo, a curiosidade √© alta (Explora tudo).
*   No final, a curiosidade √© baixa (Foca em vencer/farmar).`,
        visualState: "epsilon_greedy",
        type: "content"
      },
      {
        id: "architecture",
        title: "Arquitetura do Projeto üèóÔ∏è",
        content: `Agora vamos para o c√≥digo! Nossa arquitetura ser√° dividida em arquivos claros:

*   üìÑ **ambiente.py**: As regras do jogo (tabuleiro, vit√≥rias). O "Servidor".
*   üß† **agente.py**: O c√©rebro. Cont√©m a Q-Table e a l√≥gica de aprendizado.
*   üèãÔ∏è **treinador.py**: O "Gym". Coloca a IA para jogar contra si mesma milhares de vezes.
*   üéÆ **jogar.py**: Onde voc√™ desafia sua cria√ß√£o.

Pronto para codar?`,
        visualState: "architecture",
        type: "content"
      },
      {
        id: "quiz_phase2",
        title: "Desafio Final: Q-Learning üß†",
        content: "Voc√™ precisa acertar pelo menos **75%** das quest√µes para desbloquear a Fase 3.",
        visualState: "quiz_static",
        type: "quiz",
        quizData: [
            {
                id: "q2_1",
                question: "Qual √© o objetivo principal do algoritmo Q-Learning?",
                options: [
                    "Construir uma tabela (Q-Table) que mapeia a 'qualidade' de cada a√ß√£o em cada estado.",
                    "Memorizar sequ√™ncias de jogadas vencedoras para repeti-las exatamente da mesma forma.",
                    "Aprender as regras do jogo a partir do zero, sem nenhuma informa√ß√£o pr√©via sobre o ambiente.",
                    "Diminuir a velocidade do jogo para que o agente tenha mais tempo para tomar uma decis√£o."
                ],
                correctIndex: 0,
                explanation: "O Q-Learning busca preencher a Q-Table com valores que estimam o retorno futuro de cada par Estado-A√ß√£o."
            },
            {
                id: "q2_2",
                question: "Na Equa√ß√£o de Bellman, o que o par√¢metro Œ± (alpha) representa?",
                options: [
                    "O qu√£o 'vision√°rio' o jogador √©, valorizando mais o futuro do que o ganho imediato.",
                    "A frequ√™ncia com que o jogador decide 'explorar o mapa' em vez de 'seguir o guia'.",
                    "O qu√£o 'teimoso' ou 'impulsivo' o jogador √© ao aprender com uma nova experi√™ncia.",
                    "A recompensa final que o jogador recebe ao vencer a partida ou completar a miss√£o."
                ],
                correctIndex: 2,
                explanation: "Alpha √© a Taxa de Aprendizado: define o peso da nova informa√ß√£o em rela√ß√£o ao conhecimento antigo."
            },
            {
                id: "q2_3",
                question: "O que o par√¢metro Œ≥ (gamma), ou Fator de Desconto, controla na estrat√©gia da IA?",
                options: [
                    "A import√¢ncia que a IA d√° para as recompensas futuras em compara√ß√£o com as recompensas imediatas.",
                    "A velocidade com que a IA atualiza sua Tabela Q ap√≥s cada jogada realizada.",
                    "A probabilidade de a IA escolher uma a√ß√£o completamente aleat√≥ria durante o treinamento.",
                    "O n√∫mero m√°ximo de estados que a IA consegue armazenar em sua Tabela Q."
                ],
                correctIndex: 0,
                explanation: "Gamma define o horizonte de planejamento: quanto mais pr√≥ximo de 1, mais a IA valoriza o futuro."
            },
            {
                id: "q2_4",
                question: "O que significa a estrat√©gia 'Epsilon-Greedy'?",
                options: [
                    "A IA sempre escolhe a a√ß√£o com o maior valor Q, sendo 'gananciosa' (greedy) o tempo todo.",
                    "Um m√©todo para equilibrar entre explorar novas jogadas (aleat√≥rias) e aproveitar o conhecimento j√° adquirido.",
                    "Uma t√©cnica para reduzir o tamanho da Tabela Q, economizando mem√≥ria durante o treinamento.",
                    "A IA escolhe a a√ß√£o que leva √† menor puni√ß√£o poss√≠vel, evitando qualquer tipo de risco."
                ],
                correctIndex: 1,
                explanation: "√â a estrat√©gia cl√°ssica para resolver o dilema Explora√ß√£o vs Exploitation."
            },
            {
                id: "q2_5",
                question: "No in√≠cio do treinamento, o valor de Œµ (epsilon) deve ser alto. Por qu√™?",
                options: [
                    "Para for√ßar a IA a usar apenas as melhores jogadas conhecidas desde o come√ßo.",
                    "Para que a IA aprenda mais r√°pido, pois um epsilon alto aumenta a taxa de aprendizado.",
                    "Para encorajar a IA a explorar muitas jogadas diferentes, j√° que sua Q-Table inicial √© in√∫til.",
                    "Para garantir que a IA valorize mais as recompensas futuras e planeje a longo prazo."
                ],
                correctIndex: 2,
                explanation: "No come√ßo, a IA n√£o sabe nada, ent√£o precisa testar tudo (explora√ß√£o alta)."
            },
            {
                id: "q2_6",
                question: "Em nossa estrutura de projeto, qual arquivo √© respons√°vel por conter as regras e a l√≥gica do jogo?",
                options: [
                    "agente.py, pois ele √© o 'c√©rebro' que precisa saber as regras para jogar.",
                    "ambiente.py, pois ele define o 'mundo' onde o agente vive, incluindo suas mec√¢nicas.",
                    "treinador.py, pois ele gerencia as partidas e precisa aplicar as regras do jogo.",
                    "jogar.py, pois o jogador humano precisa consultar as regras contidas neste arquivo."
                ],
                correctIndex: 1,
                explanation: "O Ambiente √© o detentor da f√≠sica, regras e estados do jogo."
            },
            {
                id: "q2_7",
                question: "A Q-Table, o 'c√©rebro' da nossa IA, √© implementada e gerenciada dentro de qual arquivo?",
                options: [
                    "No arquivo ambiente.py, junto com o tabuleiro do jogo.",
                    "No arquivo treinador.py, que a utiliza para guiar o aprendizado.",
                    "No arquivo agente.py, pois a Q-Table √© a representa√ß√£o do seu conhecimento.",
                    "No arquivo visualizador.py, que a exibe em formato de gr√°fico."
                ],
                correctIndex: 2,
                explanation: "O Agente guarda o conhecimento aprendido (a Q-Table)."
            },
            {
                id: "q2_8",
                question: "O que significa o termo 'self-play' no contexto do nosso treinamento?",
                options: [
                    "Permitir que um jogador humano jogue contra a IA para ensin√°-la.",
                    "Fazer a IA jogar contra si mesma para gerar uma grande quantidade de experi√™ncia.",
                    "Rodar o script 'jogar.py' para testar a vers√£o final da IA.",
                    "Um modo de jogo onde a IA apenas repete as jogadas que j√° sabe que s√£o boas."
                ],
                correctIndex: 1,
                explanation: "Self-play permite treino acelerado sem depender de humanos ou bots externos."
            },
            {
                id: "q2_9",
                question: "Se uma a√ß√£o em um determinado estado tem um Q-value de 1.0 (o valor m√°ximo), o que isso significa?",
                options: [
                    "Que essa a√ß√£o foi a mais explorada durante todo o treinamento.",
                    "Que essa a√ß√£o √© a √∫nica jogada legal poss√≠vel naquele estado do jogo.",
                    "Que essa a√ß√£o leva diretamente a um estado final de vit√≥ria com recompensa de 1.0.",
                    "Que a IA tem 100% de certeza de que essa √© uma boa jogada posicional."
                ],
                correctIndex: 2,
                explanation: "O valor Q converge para a recompensa esperada. Se √© o m√°ximo, √© o caminho da vit√≥ria."
            },
            {
                id: "q2_10",
                question: "O que √© um 'estado' no contexto do Jogo da Velha?",
                options: [
                    "A decis√£o final de quem ganhou, perdeu ou empatou a partida.",
                    "Uma 'fotografia' da configura√ß√£o atual das pe√ßas 'X' e 'O' no tabuleiro.",
                    "O n√∫mero da rodada atual, indicando quantos movimentos j√° foram feitos.",
                    "A estrat√©gia geral que o agente est√° usando para tentar vencer o jogo."
                ],
                correctIndex: 1,
                explanation: "Estado define univocamente a situa√ß√£o do jogo naquele instante."
            },
            {
                id: "q2_11",
                question: "Na Equa√ß√£o de Bellman, o termo 'max Q(s', a')' representa:",
                options: [
                    "A recompensa imediata recebida ap√≥s tomar a a√ß√£o 'a' no estado 's'.",
                    "O valor Q m√°ximo que o agente espera obter no pr√≥ximo estado do jogo.",
                    "A m√©dia de todos os valores Q para o estado atual 's'.",
                    "A probabilidade de transi√ß√£o para o pr√≥ximo estado 's' ser bem-sucedida."
                ],
                correctIndex: 1,
                explanation: "√â a estimativa otimista do futuro: 'O melhor que posso fazer a partir do pr√≥ximo passo'."
            },
            {
                id: "q2_12",
                question: "Por que o Jogo da Velha √© um bom problema para come√ßar a aprender Q-Learning?",
                options: [
                    "Porque o n√∫mero de estados e a√ß√µes √© relativamente pequeno e o treino √© r√°pido.",
                    "Porque ele n√£o possui empates, o que simplifica a defini√ß√£o das recompensas.",
                    "Porque √© imposs√≠vel para um jogador humano vencer a IA, garantindo o aprendizado.",
                    "Porque ele requer o uso de Redes Neurais complexas desde o in√≠cio do projeto."
                ],
                correctIndex: 0,
                explanation: "Com poucos estados, a Q-Table cabe na mem√≥ria e converge rapidamente."
            },
            {
                id: "q2_13",
                question: "Se aumentarmos muito o Œ≥ (gamma), para perto de 1.0, que tipo de 'personalidade' a IA desenvolve?",
                options: [
                    "Impaciente e imediatista, focando apenas na recompensa da pr√≥xima jogada.",
                    "Cautelosa e defensiva, preferindo empatar a arriscar uma derrota.",
                    "Estrategista e 'vision√°ria', dando grande import√¢ncia √†s recompensas a longo prazo.",
                    "Agressiva e exploradora, fazendo mais jogadas aleat√≥rias independente do treino."
                ],
                correctIndex: 2,
                explanation: "Gamma alto faz o agente se importar com o resultado final l√° na frente, n√£o s√≥ agora."
            },
            {
                id: "q2_14",
                question: "Qual √© a principal desvantagem de usar Q-Learning com uma Q-Table?",
                options: [
                    "Ele aprende muito devagar, mesmo em jogos simples como o Jogo da Velha.",
                    "Ele se torna invi√°vel para jogos com um n√∫mero gigantesco de estados, como o xadrez.",
                    "Ele n√£o consegue aprender a jogar contra si mesmo (self-play).",
                    "Ele s√≥ funciona para jogos de um jogador e n√£o pode ser adaptado para dois jogadores."
                ],
                correctIndex: 1,
                explanation: "A maldi√ß√£o da dimensionalidade: tabelas ficam grandes demais para jogos complexos."
            },
            {
                id: "q2_15",
                question: "O processo de ajustar o valor Q baseado na 'surpresa' (diferen√ßa entre o esperado e o real) √© o n√∫cleo de qual equa√ß√£o?",
                options: [
                    "Da Equa√ß√£o de Bellman, que guia a atualiza√ß√£o da experi√™ncia do agente.",
                    "Da estrat√©gia Epsilon-Greedy, que decide quando explorar ou aproveitar.",
                    "Do Teorema de Pit√°goras, que calcula a dist√¢ncia entre dois estados.",
                    "Da Lei de Moore, que prev√™ o aumento da complexidade dos jogos."
                ],
                correctIndex: 0,
                explanation: "A equa√ß√£o de Bellman usa o erro de predi√ß√£o temporal (TD Error) para aprender."
            }
        ]
      }
    ]
  },
  {
    id: 3,
    title: "O Agente no Labirinto",
    description: "Navega√ß√£o espacial, Tuplas e Recompensas Negativas.",
    steps: [
      {
        id: "intro_maze",
        title: "O Agente no Labirinto üìç",
        content: `Bem-vindo √† Fase 3! Se na fase anterior nossa IA aprendeu a jogar um jogo de tabuleiro est√°tico, agora ela vai aprender a **andar**.

**O que vamos construir?**
Uma Intelig√™ncia Artificial que aprende a navegar em um **Labirinto (Grid World)** para encontrar a sa√≠da o mais r√°pido poss√≠vel, evitando paredes e buracos.

1.  **O Agente:** Um rob√¥ que pode se mover para Cima, Baixo, Esquerda e Direita.
2.  **O Ambiente:** Um labirinto 10x10.
3.  **O Objetivo:** Encontrar o caminho mais curto at√© a sa√≠da.`,
        visualState: "intro_maze",
        type: "content"
      },
      {
        id: "grid_state",
        title: "O Mundo em Grade üåê",
        content: `Este √© o "Hello World" da rob√≥tica. Diferente do Jogo da Velha, onde o estado era complexo, aqui o estado √© apenas a **coordenada (Linha, Coluna)** do nosso agente.

\`\`\`
  0 1 2 3
0 . . . .
1 . A . .  <-- O Agente (A) est√° na linha 1, coluna 1.
2 . . . .      Estado = (1, 1)
3 . . S .
\`\`\`

> **Conceito: Tupla**
> √â como uma "lista" imut√°vel. Pense nela como um pacote fechado de informa√ß√µes. A coordenada \`(1, 1)\` √© uma tupla √∫nica. Se voc√™ mudar de linha, vira uma tupla totalmente nova \`(2, 1)\`.`,
        visualState: "state_coords",
        type: "content"
      },
      {
        id: "grid_actions",
        title: "A√ß√µes e Movimento üéÆ",
        content: `Em vez de "marcar X na posi√ß√£o 5", nossas a√ß√µes agora s√£o movimentos f√≠sicos no espa√ßo:

*   ‚¨ÜÔ∏è **Cima** (Up)
*   ‚¨áÔ∏è **Baixo** (Down)
*   ‚¨ÖÔ∏è **Esquerda** (Left)
*   ‚û°Ô∏è **Direita** (Right)

O agente precisa decidir qual dessas 4 a√ß√µes tomar em cada quadrado do labirinto.`,
        visualState: "actions_arrows",
        type: "content"
      },
      {
        id: "living_cost",
        title: "A Pregui√ßa Inteligente ü¶•",
        content: `Como ensinamos a IA a ter pressa? Simples: **cobramos "energia" por cada passo.**

Se o agente ganhar o mesmo pr√™mio chegando em 10 passos ou 1000 passos, ele pode ficar andando em c√≠rculos.

**Sistema de Recompensas:**
*   **Chegar na Sa√≠da:** +10.0 (O Grande Biscoito üç™)
*   **Bater na Parede:** -0.5 (Dor de Cabe√ßa ü§ï)
*   **Dar um Passo:** -0.1 (Cansa√ßo üòÆ‚Äçüí®)

> **A L√≥gica da IA:** "Cada passo me custa 0.1. Se eu demorar, perco muitos pontos! Preciso correr para a sa√≠da."

Isso for√ßa a IA a encontrar o **caminho √≥timo** matematicamente.`,
        visualState: "rewards_cost",
        type: "content"
      },
      {
        id: "q_table_nav",
        title: "A Q-Table do Labirinto üß©",
        content: `Nossa "cola" (Q-Table) agora mapeia cada **quadrado do ch√£o** para as 4 dire√ß√µes.

Imagine que o ch√£o tem setas invis√≠veis indicando a qualidade de ir para cada lado.

**Exemplo para o Estado (1, 1):**

| Estado | Cima ‚¨ÜÔ∏è | Baixo ‚¨áÔ∏è | Esq ‚¨ÖÔ∏è | Dir ‚û°Ô∏è |
| :--- | :---: | :---: | :---: | :---: |
| **(1, 1)** | -0.5 | -0.1 | -0.5 | **0.8** |

*   **Cima/Esq (-0.5):** Paredes! Ruim.
*   **Baixo (-0.1):** Caminho livre, mas afasta do objetivo.
*   **Direita (0.8):** Aproxima da sa√≠da! **Melhor A√ß√£o.**`,
        visualState: "q_table_nav",
        type: "content"
      },
      {
        id: "generalization",
        title: "Do Tabuleiro para o Mapa üöÄ",
        content: `A grande mudan√ßa mental √© perceber que **RL serve para qualquer coisa**.

*   No Jogo da Velha, naveg√°vamos por estados de pe√ßas.
*   No Labirinto, navegamos por posi√ß√µes f√≠sicas.
*   No Ragnarok (futuro), navegaremos por mapas reais do jogo.

A matem√°tica (Q-Learning, Bellman) √© **exatamente a mesma**. S√≥ mudamos o que chamamos de "Estado" e "A√ß√£o".`,
        visualState: "architecture_phase3",
        type: "content"
      },
      {
        id: "quiz_phase3",
        title: "Desafio Final: O Labirinto üèÅ",
        content: "Prove que voc√™ sabe guiar o agente at√© a vit√≥ria!",
        visualState: "quiz_static",
        type: "quiz",
        quizData: [
            {
                id: "q3_1",
                question: "Em um Grid World (mundo em grade), como definimos o 'Estado'?",
                options: [
                    "Pela coordenada (Linha, Coluna) onde o agente se encontra.",
                    "Pela quantidade de passos que o agente j√° deu.",
                    "Pelo n√∫mero de inimigos presentes no mapa.",
                    "Pela dire√ß√£o para onde o agente est√° olhando."
                ],
                correctIndex: 0,
                explanation: "Em navega√ß√£o espacial, a localiza√ß√£o exata (coordenadas) define o estado atual."
            },
            {
                id: "q3_2",
                question: "O que √© uma 'Tupla' no contexto da programa√ß√£o do nosso estado?",
                options: [
                    "Um tipo de vari√°vel que muda seu valor aleatoriamente.",
                    "Uma lista imut√°vel de valores, usada aqui para agrupar (Linha, Coluna).",
                    "Uma fun√ß√£o que calcula a dist√¢ncia at√© o objetivo.",
                    "Um erro de c√≥digo que acontece quando o agente bate na parede."
                ],
                correctIndex: 1,
                explanation: "Tuplas s√£o como pacotes fechados de dados. (1, 1) √© uma tupla que representa uma posi√ß√£o √∫nica."
            },
            {
                id: "q3_3",
                question: "Por que aplicamos uma penalidade pequena (ex: -0.1) a cada passo do agente?",
                options: [
                    "Para fazer o agente desistir se o caminho for muito longo.",
                    "Para incentivar a 'pregui√ßa inteligente': encontrar o caminho mais curto para parar de perder pontos.",
                    "Porque o computador gasta energia el√©trica para calcular cada passo.",
                    "Para simular que o ch√£o √© feito de lava e o agente morre se andar muito."
                ],
                correctIndex: 1,
                explanation: "O 'living cost' for√ßa a otimiza√ß√£o. Sem ele, o agente poderia andar em c√≠rculos infinitamente sem preju√≠zo."
            },
            {
                id: "q3_4",
                question: "Se a Q-Table para o estado (1,1) diz: Direita=0.8 e Baixo=-0.1, o que o agente deve fazer?",
                options: [
                    "Ir para Baixo, pois n√∫meros negativos s√£o melhores em RL.",
                    "Ficar parado, pois nenhum n√∫mero √© 1.0.",
                    "Ir para a Direita, pois √© a a√ß√£o com maior valor Q (maior qualidade).",
                    "Escolher aleatoriamente, pois a diferen√ßa √© pequena."
                ],
                correctIndex: 2,
                explanation: "O Agente (em modo exploitation) sempre escolhe o maior valor Q."
            },
            {
                id: "q3_5",
                question: "Qual a diferen√ßa fundamental na matem√°tica do Q-Learning entre o Jogo da Velha e o Labirinto?",
                options: [
                    "Nenhuma. A matem√°tica √© exatamente a mesma, s√≥ mudam as defini√ß√µes de Estado e A√ß√£o.",
                    "No Labirinto usamos a Equa√ß√£o de Bellman Inversa.",
                    "No Jogo da Velha n√£o existe Q-Table, apenas no Labirinto.",
                    "O Labirinto requer Deep Learning, enquanto Jogo da Velha usa Machine Learning simples."
                ],
                correctIndex: 0,
                explanation: "O algoritmo √© agn√≥stico ao problema. Se voc√™ consegue definir Estado, A√ß√£o e Recompensa, o Q-Learning funciona."
            }
        ]
      }
    ]
  }
];