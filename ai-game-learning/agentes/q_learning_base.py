import numpy as np
import pickle
import random
from abc import ABC, abstractmethod
from typing import Dict, List, Tuple, TypeVar, Generic, Optional, Any

# Definimos um tipo gen√©rico para o Estado, pois ele pode mudar de jogo para jogo.
# No Jogo da Velha √© uma Tupla de 9 inteiros. No Labirinto √© uma Tupla (linha, coluna).

Estado = TypeVar('Estado')


class AgenteQLearningBase(ABC, Generic[Estado]):
    """
    Classe Base para Agentes de Q-Learning (Aprendizado por Refor√ßo).

    Esta classe implementa a l√≥gica central do algoritmo Q-Learning de forma gen√©rica,
    permitindo que ela seja reutilizada em diferentes jogos (Jogo da Velha, Labirinto, etc).

    ---
    üß† O QUE √â Q-LEARNING? (Conceito R√°pido)

    Imagine que voc√™ tem um caderno (Q-Table). Em cada p√°gina desse caderno, voc√™ anota
    uma situa√ß√£o do jogo (Estado). Nessa p√°gina, voc√™ lista todas as a√ß√µes poss√≠veis
    e d√° uma nota (Valor Q) para cada uma.

    - No come√ßo, todas as notas s√£o 0 (voc√™ n√£o sabe nada).
    - Voc√™ joga e v√™ o que acontece.
    - Se uma a√ß√£o te levou √† vit√≥ria, voc√™ aumenta a nota dela.
    - Se te levou √† derrota, voc√™ diminui.

    Com o tempo, seu caderno se torna um guia perfeito de "o que fazer em cada situa√ß√£o".
    Esta classe √© o "gerenciador desse caderno".
    ---
    """

    def __init__(
        self,
        alpha: float = 0.1,
        gamma: float = 0.9,
        epsilon: float = 1.0,
        epsilon_min: float = 0.01,
        epsilon_decay: float = 0.995
    ):
        """
        Inicializa o Agente Q-Learning.

        Args:
            alpha (float): Taxa de Aprendizado (0.0 a 1.0).
                           O quanto o agente substitui o conhecimento antigo pelo novo.
                           - 0.0: N√£o aprende nada novo.
                           - 1.0: Esquece tudo e s√≥ confia na √∫ltima experi√™ncia.

            gamma (float): Fator de Desconto (0.0 a 1.0).
                           O quanto o agente valoriza recompensas futuras.
                           - 0.0: M√≠ope (s√≥ liga para a recompensa imediata).
                           - 1.0: Vision√°rio (valoriza recompensas a longo prazo igualmente).

            epsilon (float): Taxa de Explora√ß√£o Inicial (0.0 a 1.0).
                             Probabilidade de fazer uma a√ß√£o aleat√≥ria para descobrir coisas novas.

            epsilon_min (float): Valor m√≠nimo para o epsilon.
                                 Garante que o agente nunca pare de explorar totalmente (opcional).

            epsilon_decay (float): Fator de decaimento do epsilon a cada epis√≥dio.
                                   Reduz a explora√ß√£o gradualmente conforme o agente aprende.
        """
        self.alpha = alpha
        self.gamma = gamma
        self.epsilon = epsilon
        self.epsilon_min = epsilon_min
        self.epsilon_decay = epsilon_decay

        # A Q-Table √© o nosso "c√©rebro".
        # Mapeia: Estado -> {A√ß√£o: Valor Q}
        # Exemplo: (0,0) -> { 'cima': 0.5, 'baixo': -0.1 }
        self.q_table: Dict[Estado, Dict[Any, float]] = {}

        # Estat√≠sticas para acompanhamento
        self.treinamento_epocas = 0

    def obter_valor_q(self, estado: Estado, acao: Any) -> float:
        """
        Retorna o valor Q para um determinado par (estado, a√ß√£o).

        Se o estado ou a a√ß√£o n√£o existirem na tabela, retorna 0.0.
        """
        if estado not in self.q_table:
            return 0.0
        return self.q_table[estado].get(acao, 0.0)

    def escolher_acao(self, estado: Estado, acoes_possiveis: List[Any]) -> Any:
        """
        Escolhe uma a√ß√£o baseada na pol√≠tica Epsilon-Greedy.

        PENSAMENTO DO SENIOR:
        "Devo explorar ou aproveitar o que j√° sei?"

        1. Explora√ß√£o (Exploration): Com chance 'epsilon', escolho qualquer coisa.
           Isso √© vital para n√£o ficar preso em estrat√©gias ruins s√≥ porque deram sorte uma vez.

        2. Aproveitamento (Exploitation): Caso contr√°rio, escolho a MELHOR a√ß√£o conhecida.
           Olho na minha Q-Table e vejo qual a√ß√£o tem a maior nota.

        """

        # 1. Explora√ß√£o
        if random.random() < self.epsilon:
            return random.choice(acoes_possiveis)

        # 2. Aproveitamento (Exploitation)
        return self._obter_melhor_acao(estado, acoes_possiveis)

    def _obter_melhor_acao(self, estado: Estado, acoes_possiveis: List[Any]) -> Any:
        """
        Retorna a a√ß√£o com o maior valor Q para o estado.
        Se houver empate, escolhe aleatoriamente entre as melhores.
        """
        # Se o estado √© novo, qualquer a√ß√£o vale 0.0, ent√£o escolha aleat√≥ria
        if estado not in self.q_table:
            return random.choice(acoes_possiveis)

        # Obt√©m os valores Q para as a√ß√µes poss√≠veis
        valores_q = {acao: self.obter_valor_q(
            estado, acao) for acao in acoes_possiveis}

        # Encontra o valor m√°ximo
        max_q = max(valores_q.values())

        # Encontra todas as a√ß√µes que t√™m esse valor m√°ximo (para desempate)
        melhores_acoes = [acao for acao, q in valores_q.items() if q == max_q]

        return random.choice(melhores_acoes)

    def atualizar_q_table(
        self,
        estado: Estado,
        acao: Any,
        recompensa: float,
        proximo_estado: Estado,
        acoes_proximo_estado: List[Any],
        finalizado: bool = False
    ):
        """
        Atualiza a Q-Table usando a Equa√ß√£o de Bellman (Temporal Difference Learning).

        F√≥rmula M√°gica:
        Q(s,a) = Q(s,a) + alpha * (R + gamma * max(Q(s',a')) - Q(s,a))

        PENSAMENTO DO SENIOR:
        "O valor de uma a√ß√£o √© o que eu ganhei agora (R) mais a minha melhor estimativa
        do que vou ganhar no futuro (max Q do pr√≥ximo estado), tudo isso ajustado pela
        minha taxa de aprendizado (alpha)."

        Se o jogo acabou (finalizado=True), n√£o existe futuro, ent√£o o termo 'gamma * max(...)' some.

        """
        # Garante que o estado existe na tabela
        if estado not in self.q_table:
            self.q_table[estado] = {}

        q_atual = self.obter_valor_q(estado, acao)

        # Calcula o valor m√°ximo esperado para o pr√≥ximo estado
        if finalizado:
            max_q_futuro = 0.0
        else:
            # Se o pr√≥ximo estado n√£o foi visitado, seus valores s√£o 0.0
            valores_futuros = [self.obter_valor_q(
                proximo_estado, a) for a in acoes_proximo_estado]
            max_q_futuro = max(valores_futuros) if valores_futuros else 0.0

        # Equa√ß√£o de Bellman
        novo_q = q_atual + self.alpha * \
            (recompensa + (self.gamma * max_q_futuro) - q_atual)

        # Atualiza a tabela
        self.q_table[estado][acao] = novo_q

    def decair_epsilon(self):
        """
        Reduz a taxa de explora√ß√£o ap√≥s cada epis√≥dio.
        Isso faz o agente ficar mais "s√©rio" e confiar mais no que aprendeu conforme o tempo passa.
        """
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay

    def salvar_modelo(self, caminho_arquivo: str):
        """Salva a Q-Table em um arquivo pickle."""
        try:
            with open(caminho_arquivo, 'wb') as f:
                pickle.dump(self.q_table, f)
            print(f"üíæ Modelo salvo com sucesso em: {caminho_arquivo}")
        except Exception as e:
            print(f"‚ùå Erro ao salvar modelo: {e}")

    def carregar_modelo(self, caminho_arquivo: str):
        """Carrega a Q-Table de um arquivo pickle."""
        try:
            with open(caminho_arquivo, 'rb') as f:
                self.q_table = pickle.load(f)
            print(f"üìÇ Modelo carregado de: {caminho_arquivo}")
        except FileNotFoundError:
            print(
                f"‚ö†Ô∏è Arquivo {caminho_arquivo} n√£o encontrado. Iniciando com Q-Table vazia.")
        except Exception as e:
            print(f"‚ùå Erro ao carregar modelo: {e}")
