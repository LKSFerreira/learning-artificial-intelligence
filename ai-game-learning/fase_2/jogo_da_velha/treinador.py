"""
M√≥dulo: üèõÔ∏è treinador.py
Projeto: üìò AI Game Learning

Este m√≥dulo implementa o Treinador, que √© respons√°vel por orquestrar o treinamento
de dois agentes Q-Learning atrav√©s de self-play (autoaprendizado).

O Treinador funciona como um "Mestre da Guilda" ou "Dungeon Master" que:
- Coordena milhares de partidas entre os dois agentes
- Gerencia o ciclo completo de treinamento (partida ‚Üí aprendizado ‚Üí estat√≠sticas)
- Fornece interfaces visuais ricas para acompanhamento em tempo real
- Salva checkpoints peri√≥dicos e modelos finais treinados
- Avalia o desempenho dos agentes ap√≥s o treinamento

O processo de self-play √© fundamental para o aprendizado: os agentes jogam
uns contra os outros, aprendendo com cada vit√≥ria, derrota e empate. Com o
tempo, ambos os agentes melhoram simultaneamente, criando um ciclo virtuoso
de aprendizado.

Este m√≥dulo suporta duas interfaces visuais:
- Rich (avan√ßada): Interface rica com pain√©is, tabelas e atualiza√ß√µes em tempo real
- TQDM (b√°sica): Barra de progresso simples, usada como fallback
"""

from datetime import datetime
import os
from pathlib import Path
from typing import Tuple, Dict, List, Optional

# Tenta importar a biblioteca 'rich' para uma interface visual avan√ßada.
# Se n√£o estiver instalada, define RICH_DISPONIVEL como False.
try:
    from rich.live import Live
    from rich.table import Table
    from rich.panel import Panel
    from rich.progress import Progress, BarColumn, TextColumn, TimeRemainingColumn
    RICH_DISPONIVEL = True
except ImportError:
    RICH_DISPONIVEL = False

# Tqdm √© usado como uma alternativa mais simples se 'rich' n√£o estiver dispon√≠vel.
from tqdm import tqdm

from .ambiente import AmbienteJogoDaVelha
from .agente import AgenteQLearning


class Treinador:
    """
    Orquestra o treinamento de dois agentes Q-Learning atrav√©s de self-play.

    O Treinador coordena o processo completo de treinamento, gerenciando:
    - Execu√ß√£o de partidas entre os agentes
    - Aplica√ß√£o de recompensas e aprendizado
    - Salvamento de checkpoints peri√≥dicos
    - Exibi√ß√£o de estat√≠sticas em tempo real
    - Avalia√ß√£o do desempenho final

    O self-play funciona assim:
    1. Os dois agentes jogam uma partida completa
    2. No final, cada agente recebe uma recompensa baseada no resultado
    3. Os agentes aprendem usando o m√©todo Monte Carlo
    4. O processo se repete milhares de vezes
    5. Com o tempo, ambos os agentes melhoram simultaneamente

    Attributes:
        agente_x (AgenteQLearning): Agente que joga como 'X' (jogador 1).
        agente_o (AgenteQLearning): Agente que joga como 'O' (jogador 2).
        ambiente (AmbienteJogoDaVelha): Ambiente do jogo onde as partidas ocorrem.
        pasta_modelos (Path): Caminho da pasta onde os modelos s√£o salvos.
        _checkpoints (List[Dict]): Lista de metadados dos checkpoints salvos.

    Example:
        >>> ambiente = AmbienteJogoDaVelha(dimensao=3)
        >>> agente_x = AgenteQLearning(jogador=1)
        >>> agente_o = AgenteQLearning(jogador=2)
        >>> treinador = Treinador(agente_x, agente_o, ambiente)
        >>> treinador.treinar(numero_de_partidas=10000)
    """

    def __init__(self, agente_x: AgenteQLearning, agente_o: AgenteQLearning, ambiente: AmbienteJogoDaVelha):
        """
        Inicializa uma nova inst√¢ncia do Treinador.

        Args:
            agente_x: Agente que jogar√° como 'X' (jogador 1).
            agente_o: Agente que jogar√° como 'O' (jogador 2).
            ambiente: Ambiente do jogo onde as partidas ser√£o executadas.

        Note:
            A pasta de modelos ser√° criada automaticamente se n√£o existir.
            Os checkpoints s√£o inicializados como uma lista vazia.
        """
        self.agente_x = agente_x
        self.agente_o = agente_o
        self.ambiente = ambiente
        self.pasta_modelos = Path("modelos_treinados")
        self.pasta_modelos.mkdir(exist_ok=True)
        self._checkpoints: List[Dict] = []  # Lista para armazenar metadados dos checkpoints

    def executar_uma_partida(self) -> int:
        """
        Executa uma √∫nica partida completa entre os dois agentes.

        Este m√©todo coordena uma partida do in√≠cio ao fim:
        1. Reinicia o ambiente e limpa os hist√≥ricos dos agentes
        2. Alterna entre os agentes at√© a partida terminar
        3. Calcula as recompensas baseadas no resultado
        4. Aplica o aprendizado Monte Carlo em ambos os agentes

        O processo de self-play acontece aqui: os agentes jogam uns contra
        os outros, registrando suas jogadas e aprendendo no final.

        Returns:
            Identificador do vencedor: 1 (agente X), 2 (agente O), ou 0 (empate).

        Note:
            Este m√©todo n√£o modifica os par√¢metros de treinamento dos agentes
            (como epsilon), apenas executa a partida e aplica o aprendizado.
        """
        # Reinicia o ambiente para uma nova partida
        self.ambiente.reiniciar_partida()
        
        # Limpa os hist√≥ricos dos agentes para come√ßar uma nova partida
        self.agente_x.limpar_historico_partida()
        self.agente_o.limpar_historico_partida()

        # Loop principal da partida: continua at√© algu√©m vencer ou empatar
        while not self.ambiente.partida_finalizada:
            # Determina qual agente deve jogar nesta rodada
            agente_atual = self.agente_x if self.ambiente.jogador_atual == 1 else self.agente_o
            
            # Obt√©m o estado atual do tabuleiro e as a√ß√µes v√°lidas
            estado_atual = self.ambiente.obter_estado_como_tupla()
            acoes_validas = self.ambiente.obter_acoes_validas()
            
            # O agente escolhe uma a√ß√£o usando sua estrat√©gia Epsilon-Greedy
            acao_escolhida = agente_atual.escolher_acao(estado_atual, acoes_validas, em_treinamento=True)
            
            # Registra a jogada no hist√≥rico para aprendizado Monte Carlo posterior
            agente_atual.adicionar_jogada_ao_historico(estado_atual, acao_escolhida)
            
            # Executa a a√ß√£o escolhida no ambiente
            self.ambiente.executar_jogada(acao_escolhida)

        # Calcula as recompensas baseadas no resultado final
        if self.ambiente.vencedor == 1:
            # Agente X venceu: recompensa positiva para X, negativa para O
            recompensa_x, recompensa_o = 1.0, -1.0
        elif self.ambiente.vencedor == 2:
            # Agente O venceu: recompensa positiva para O, negativa para X
            recompensa_x, recompensa_o = -1.0, 1.0
        else:
            # Empate: recompensa neutra para ambos
            recompensa_x, recompensa_o = 0.0, 0.0
            
        # Aplica o aprendizado Monte Carlo em ambos os agentes
        # Cada agente aprende com base no hist√≥rico de sua partida
        self.agente_x.processar_aprendizado_monte_carlo(recompensa_x)
        self.agente_o.processar_aprendizado_monte_carlo(recompensa_o)
        
        return self.ambiente.vencedor

    def treinar(self, numero_de_partidas: int = 50000, intervalo_log: int = 1000, intervalo_checkpoint: int = 10000):
        """
        Executa o loop principal de treinamento com interface visual em tempo real.

        Este m√©todo coordena todo o processo de treinamento, executando milhares
        de partidas e fornecendo feedback visual sobre o progresso. Ele suporta
        duas interfaces: Rich (avan√ßada) e TQDM (b√°sica).

        O treinamento funciona atrav√©s de self-play: os agentes jogam uns contra
        os outros repetidamente, melhorando gradualmente com cada partida.

        Args:
            numero_de_partidas: N√∫mero total de partidas a serem executadas.
                Padr√£o: 50.000. Valores t√≠picos: 10.000 a 500.000.
            intervalo_log: Intervalo (em partidas) para resetar a janela de
                estat√≠sticas. Padr√£o: 1.000. Usado para calcular taxas recentes.
            intervalo_checkpoint: Intervalo (em partidas) para salvar checkpoints.
                Padr√£o: 10.000. Permite recuperar o treinamento se interrompido.

        Note:
            - Checkpoints s√£o salvos automaticamente nos intervalos especificados
            - As estat√≠sticas s√£o exibidas em tempo real durante o treinamento
            - Ao final, os modelos finais s√£o salvos automaticamente
            - O m√©todo detecta automaticamente se Rich est√° dispon√≠vel

        Example:
            >>> treinador = Treinador(agente_x, agente_o, ambiente)
            >>> treinador.treinar(
            ...     numero_de_partidas=100000,
            ...     intervalo_log=5000,
            ...     intervalo_checkpoint=20000
            ... )
        """
        print("\n" + "="*50)
        print("‚öîÔ∏è INICIANDO TREINAMENTO INTENSIVO (SELF-PLAY) ‚öîÔ∏è")
        print("="*50)
        print(f"Total de Partidas: {numero_de_partidas:,}")
        print(f"Interface Gr√°fica: {'Rich (Avan√ßada)' if RICH_DISPONIVEL else 'TQDM (B√°sica)'}")
        print("="*50 + "\n")

        # Inicializa lista de checkpoints e contadores de estat√≠sticas
        self._checkpoints = []
        vitorias_x_janela, vitorias_o_janela, empates_janela = 0, 0, 0
        ultimo_checkpoint: Optional[str] = None

        if RICH_DISPONIVEL:
            # --- MODO RICH (Interface Avan√ßada) ---
            # Cria uma barra de progresso com informa√ß√µes detalhadas
            progresso = Progress(
                TextColumn("[bold blue]{task.description}"), 
                BarColumn(), 
                TextColumn("{task.percentage:>3.0f}%"), 
                TimeRemainingColumn()
            )
            id_tarefa = progresso.add_task("Treinando", total=numero_de_partidas)

            def gerar_painel_estatisticas() -> Panel:
                """
                Gera um painel com estat√≠sticas em tempo real do treinamento.

                As estat√≠sticas incluem:
                - Vit√≥rias, derrotas e empates na janela atual
                - Taxa de empate (indicador de qualidade do treinamento)
                - Epsilon atual de cada agente (taxa de explora√ß√£o)
                - N√∫mero de estados conhecidos (tamanho da tabela Q)
                - Informa√ß√µes sobre o √∫ltimo checkpoint salvo
                """
                tabela = Table.grid(expand=True)
                tabela.add_column(justify="left")
                tabela.add_column(justify="right")
                
                # Calcula o total para evitar divis√£o por zero
                total_janela = vitorias_x_janela + vitorias_o_janela + empates_janela or 1
                
                # Estat√≠sticas da janela atual (√∫ltimas N partidas)
                tabela.add_row("Vit√≥rias X (janela)", f"[bold green]{vitorias_x_janela}[/]")
                tabela.add_row("Vit√≥rias O (janela)", f"[bold yellow]{vitorias_o_janela}[/]")
                tabela.add_row("Empates (janela)", f"{empates_janela}")
                tabela.add_row("Taxa de Empate %", f"{(empates_janela / total_janela) * 100:.1f}%")
                tabela.add_row("-" * 20, "-" * 20)
                
                # Par√¢metros de treinamento
                tabela.add_row("Epsilon X", f"{self.agente_x.epsilon:.6f}")
                tabela.add_row("Epsilon O", f"{self.agente_o.epsilon:.6f}")
                
                # Conhecimento adquirido
                tabela.add_row("Estados Conhecidos X", f"{len(self.agente_x.tabela_q):,}")
                tabela.add_row("Estados Conhecidos O", f"{len(self.agente_o.tabela_q):,}")
                
                # Informa√ß√µes de checkpoint
                tabela.add_row("√öltimo Checkpoint", f"{ultimo_checkpoint or 'Nenhum'}")
                
                return Panel(tabela, title="[bold]Estat√≠sticas da Janela[/]", border_style="blue")

            def gerar_layout():
                """
                Gera o layout completo da interface Rich.

                O layout combina a barra de progresso com o painel de estat√≠sticas,
                criando uma interface visual rica e informativa.
                """
                layout = Table.grid(expand=True)
                layout.add_row(
                    Panel(progresso, title="[bold]Progresso Geral[/]", border_style="green"), 
                    gerar_painel_estatisticas()
                )
                return layout

            # Loop principal de treinamento com interface Rich
            with Live(gerar_layout(), refresh_per_second=10) as live:
                for indice_partida in range(numero_de_partidas):
                    # Executa uma partida completa
                    vencedor = self.executar_uma_partida()
                    
                    # Atualiza contadores da janela de estat√≠sticas
                    if vencedor == 1: 
                        vitorias_x_janela += 1
                    elif vencedor == 2: 
                        vitorias_o_janela += 1
                    else: 
                        empates_janela += 1

                    # Atualiza a barra de progresso
                    progresso.update(id_tarefa, advance=1)

                    # Reseta a janela de estat√≠sticas no intervalo especificado
                    if (indice_partida + 1) % intervalo_log == 0:
                        vitorias_x_janela, vitorias_o_janela, empates_janela = 0, 0, 0
                    
                    # Salva checkpoint no intervalo especificado
                    if (indice_partida + 1) % intervalo_checkpoint == 0:
                        self._salvar_checkpoint(indice_partida + 1)
                        ultimo_checkpoint = f"{indice_partida+1:,}"

                    # Atualiza o layout completo periodicamente para melhor performance
                    if indice_partida % 250 == 0:
                        live.update(gerar_layout())
                
                # Atualiza√ß√£o final para garantir que tudo est√° vis√≠vel
                live.update(gerar_layout())
        else:
            # --- MODO TQDM (Interface B√°sica) ---
            # Fallback para quando Rich n√£o est√° dispon√≠vel
            for indice_partida in tqdm(range(numero_de_partidas), desc="Treinando"):
                vencedor = self.executar_uma_partida()
                
                # Atualiza contadores (n√£o exibidos em TQDM, mas mantidos para consist√™ncia)
                if vencedor == 1: 
                    vitorias_x_janela += 1
                elif vencedor == 2: 
                    vitorias_o_janela += 1
                else: 
                    empates_janela += 1

                # Salva checkpoint no intervalo especificado
                if (indice_partida + 1) % intervalo_checkpoint == 0:
                    self._salvar_checkpoint(indice_partida + 1)
        
        # Exibe resumo de checkpoints salvos
        self._exibir_resumo_checkpoints()
        
        print("\n" + "="*50)
        print("‚úÖ TREINAMENTO CONCLU√çDO!")
        print("="*50)
        
        # Exibe estat√≠sticas finais dos agentes
        self.agente_x.imprimir_estatisticas()
        self.agente_o.imprimir_estatisticas()
        
        # Salva os modelos finais
        self._salvar_modelos_finais()

    def _salvar_checkpoint(self, numero_partida: int):
        """
        Salva o estado atual dos agentes em um checkpoint e registra metadados.

        Checkpoints permitem recuperar o treinamento de um ponto espec√≠fico,
        √∫til para treinamentos longos que podem ser interrompidos. Cada checkpoint
        salva a tabela Q completa de ambos os agentes.

        Args:
            numero_partida: N√∫mero da partida atual (usado no nome do arquivo).

        Note:
            Este m√©todo registra metadados sobre o checkpoint (sucesso/falha,
            timestamp, etc.) para rastreamento posterior. Se houver erro ao salvar,
            o erro √© registrado mas n√£o interrompe o treinamento.
        """
        caminho_x = self.pasta_modelos / f"agente_x_checkpoint_{numero_partida}.pkl"
        caminho_o = self.pasta_modelos / f"agente_o_checkpoint_{numero_partida}.pkl"
        
        try:
            # Salva as tabelas Q de ambos os agentes
            self.agente_x.salvar_memoria(str(caminho_x))
            self.agente_o.salvar_memoria(str(caminho_o))
            
            # Registra metadados do checkpoint bem-sucedido
            self._checkpoints.append({
                'numero_partida': numero_partida,
                'timestamp': datetime.now(),
                'pasta': str(self.pasta_modelos),
                'sucesso': True
            })
        except Exception as e:
            # Registra falha sem interromper o treinamento
            self._checkpoints.append({
                'numero_partida': numero_partida,
                'timestamp': datetime.now(),
                'erro': str(e),
                'sucesso': False
            })

    def _salvar_modelos_finais(self):
        """
        Salva os modelos finais ap√≥s o t√©rmino do treinamento.

        Os modelos finais s√£o salvos com um nome padronizado que inclui a
        dimens√£o do tabuleiro, facilitando a identifica√ß√£o e uso posterior.

        Note:
            Este m√©todo √© chamado automaticamente ao final do treinamento.
            Os modelos s√£o salvos na pasta de modelos com nomes descritivos.
        """
        caminho_x = self.pasta_modelos / f"agente_x_final_{self.ambiente.dimensao}x{self.ambiente.dimensao}.pkl"
        caminho_o = self.pasta_modelos / f"agente_o_final_{self.ambiente.dimensao}x{self.ambiente.dimensao}.pkl"
        self.agente_x.salvar_memoria(str(caminho_x))
        self.agente_o.salvar_memoria(str(caminho_o))

    def _exibir_resumo_checkpoints(self):
        """
        Exibe um resumo organizado dos checkpoints salvos durante o treinamento.

        Este m√©todo formata e exibe informa√ß√µes sobre todos os checkpoints criados,
        incluindo sucessos e falhas, com timestamps e n√∫meros de partida formatados.

        Note:
            Se nenhum checkpoint foi salvo, exibe uma mensagem de aviso.
            Checkpoints com erro s√£o listados separadamente dos bem-sucedidos.
        """
        if not self._checkpoints or len(self._checkpoints) == 0:
            print('\n‚ö†Ô∏è  Nenhum checkpoint foi salvo.\n')
            return

        # Separa checkpoints bem-sucedidos dos que falharam
        checkpoints_sucesso = [cp for cp in self._checkpoints if cp.get('sucesso', False)]
        
        print('\n' + '‚îÅ' * 80)
        print('üíæ CHECKPOINTS SALVOS')
        print('‚îÅ' * 80 + '\n')
        
        if checkpoints_sucesso:
            print(f'‚úÖ {len(checkpoints_sucesso)} checkpoint(s) criado(s) com sucesso\n')
            print(f'üìÅ Pasta: {checkpoints_sucesso[0]["pasta"]}\n')
            
            # Exibe cada checkpoint bem-sucedido com formata√ß√£o
            for cp in checkpoints_sucesso:
                data_formatada = cp['timestamp'].strftime('%d/%m/%Y √†s %H:%M')
                partida_formatada = f"{cp['numero_partida']:,}".replace(',', '.')
                print(f'  üéØ Partida {partida_formatada} ‚Äî {data_formatada}')
        
        # Exibe erros se houver
        checkpoints_erro = [cp for cp in self._checkpoints if not cp.get('sucesso', False)]
        if checkpoints_erro:
            print(f'\n‚ùå {len(checkpoints_erro)} checkpoint(s) com erro:\n')
            for cp in checkpoints_erro:
                partida_formatada = f"{cp['numero_partida']:,}".replace(',', '.')
                print(f'  ‚ö†Ô∏è  Partida {partida_formatada} ‚Äî {cp.get("erro", "Erro desconhecido")}')
        
        print('\n' + '‚îÅ' * 80 + '\n')

    def avaliar_agentes(self, numero_de_partidas: int = 10000):
        """
        Avalia o desempenho dos agentes em modo de performance m√°xima (sem explora√ß√£o).

        Este m√©todo coloca os agentes para jogar uns contra os outros com
        epsilon=0 (sem explora√ß√£o aleat√≥ria), permitindo avaliar o conhecimento
        real adquirido durante o treinamento. √â √∫til para verificar se o
        treinamento foi bem-sucedido.

        O m√©todo tenta carregar modelos automaticamente se os agentes n√£o estiverem
        treinados, facilitando a avalia√ß√£o de modelos salvos anteriormente.

        Args:
            numero_de_partidas: N√∫mero de partidas a serem executadas para avalia√ß√£o.
                Padr√£o: 10.000. Valores maiores fornecem estat√≠sticas mais confi√°veis.

        Note:
            - Os agentes jogam com em_treinamento=False (sem explora√ß√£o)
            - Se os agentes n√£o estiverem treinados, tenta carregar modelos do disco
            - As estat√≠sticas s√£o exibidas em tempo real durante a avalia√ß√£o
            - Ao final, exibe um resumo completo dos resultados

        Example:
            >>> treinador.avaliar_agentes(numero_de_partidas=5000)
        """
        print("\n" + "="*50)
        print("üèÜ INICIANDO MODO DE AVALIA√á√ÉO (SEM EXPLORA√á√ÉO) üèÜ")
        print("="*50)

        # --- L√ìGICA DE CARREGAMENTO AUTOM√ÅTICO ---
        # Se os agentes n√£o estiverem treinados, tenta carregar modelos salvos
        if not self.agente_x.tabela_q:
            print("Agente X n√£o treinado. Tentando carregar modelo do disco...")
            caminho_x = self.pasta_modelos / f"superagente_final_{self.ambiente.dimensao}x{self.ambiente.dimensao}.pkl"
            self.agente_x = AgenteQLearning.carregar(str(caminho_x), jogador=1)

        if not self.agente_o.tabela_q:
            print("Agente O n√£o treinado. Tentando carregar modelo do disco...")
            caminho_o = self.pasta_modelos / f"agente_o_final_{self.ambiente.dimensao}x{self.ambiente.dimensao}.pkl"
            self.agente_o = AgenteQLearning.carregar(str(caminho_o), jogador=2)
        
        # Inicializa contadores de resultados
        vitorias_x, vitorias_o, empates = 0, 0, 0

        if RICH_DISPONIVEL:
            # --- MODO RICH (Interface Avan√ßada) ---
            progresso = Progress(
                TextColumn("[bold blue]{task.description}"), 
                BarColumn(), 
                TextColumn("{task.percentage:>3.0f}%"), 
                TimeRemainingColumn()
            )
            id_tarefa = progresso.add_task("Avaliando", total=numero_de_partidas)

            def gerar_painel_estatisticas_avaliacao() -> Panel:
                """
                Gera um painel com estat√≠sticas da avalia√ß√£o em tempo real.

                As estat√≠sticas incluem:
                - Vit√≥rias, derrotas e empates acumulados
                - Taxa de empate (indicador de qualidade)
                - N√∫mero de estados conhecidos por cada agente
                """
                tabela = Table.grid(expand=True)
                tabela.add_column(justify="left")
                tabela.add_column(justify="right")
                total_partidas = vitorias_x + vitorias_o + empates or 1
                
                tabela.add_row("Vit√≥rias X", f"[bold green]{vitorias_x}[/]")
                tabela.add_row("Vit√≥rias O", f"[bold yellow]{vitorias_o}[/]")
                tabela.add_row("Empates", f"{empates}")
                tabela.add_row("-" * 20, "-" * 20)
                tabela.add_row("Taxa de Empate %", f"{(empates / total_partidas) * 100:.2f}%")
                tabela.add_row("Estados Conhecidos X", f"{len(self.agente_x.tabela_q):,}")
                tabela.add_row("Estados Conhecidos O", f"{len(self.agente_o.tabela_q):,}")
                
                return Panel(tabela, title="[bold]Estat√≠sticas em Tempo Real[/]", border_style="blue")

            def gerar_layout():
                """Gera o layout completo da interface de avalia√ß√£o."""
                layout = Table.grid(expand=True)
                layout.add_row(
                    Panel(progresso, title="[bold]Progresso da Avalia√ß√£o[/]", border_style="green"), 
                    gerar_painel_estatisticas_avaliacao()
                )
                return layout

            # Loop principal de avalia√ß√£o com interface Rich
            with Live(gerar_layout(), refresh_per_second=10) as live:
                for indice_partida in range(numero_de_partidas):
                    # Reinicia o ambiente para uma nova partida
                    self.ambiente.reiniciar_partida()
                    
                    # Executa a partida sem explora√ß√£o (em_treinamento=False)
                    while not self.ambiente.partida_finalizada:
                        agente_atual = self.agente_x if self.ambiente.jogador_atual == 1 else self.agente_o
                        estado = self.ambiente.obter_estado_como_tupla()
                        acoes = self.ambiente.obter_acoes_validas()
                        # em_treinamento=False: sempre escolhe a melhor a√ß√£o conhecida
                        acao = agente_atual.escolher_acao(estado, acoes, em_treinamento=False)
                        self.ambiente.executar_jogada(acao)

                    # Atualiza contadores baseado no resultado
                    if self.ambiente.vencedor == 1: 
                        vitorias_x += 1
                    elif self.ambiente.vencedor == 2: 
                        vitorias_o += 1
                    else: 
                        empates += 1
                    
                    # Atualiza a barra de progresso
                    progresso.update(id_tarefa, advance=1)
                    
                    # Atualiza o layout periodicamente
                    if indice_partida % 250 == 0:
                        live.update(gerar_layout())
                    
                # Atualiza√ß√£o final garantida
                live.update(gerar_layout())

        else:
            # --- MODO TQDM (Interface B√°sica) ---
            # Fallback para quando Rich n√£o est√° dispon√≠vel
            for _ in tqdm(range(numero_de_partidas), desc="Avaliando Performance"):
                self.ambiente.reiniciar_partida()
                
                # Executa a partida sem explora√ß√£o
                while not self.ambiente.partida_finalizada:
                    agente_atual = self.agente_x if self.ambiente.jogador_atual == 1 else self.agente_o
                    estado = self.ambiente.obter_estado_como_tupla()
                    acoes = self.ambiente.obter_acoes_validas()
                    acao = agente_atual.escolher_acao(estado, acoes, em_treinamento=False)
                    self.ambiente.executar_jogada(acao)

                # Atualiza contadores
                if self.ambiente.vencedor == 1: 
                    vitorias_x += 1
                elif self.ambiente.vencedor == 2: 
                    vitorias_o += 1
                else: 
                    empates += 1

        # Exibe resumo final da avalia√ß√£o
        print("\n--- RESULTADO FINAL DA AVALIA√á√ÉO ---")
        print(f"Partidas Jogadas: {numero_de_partidas}")
        print(f"Vit√≥rias de X: {vitorias_x} ({(vitorias_x/numero_de_partidas)*100:.1f}%)")
        print(f"Vit√≥rias de O: {vitorias_o} ({(vitorias_o/numero_de_partidas)*100:.1f}%)")
        print(f"Empates: {empates} ({(empates/numero_de_partidas)*100:.1f}%)")
        print("="*50 + "\n")
    
    def mesclar_agentes_treinados(self):
        """
        Executa o script mesclar_modelos.py para criar o superagente.

        O superagente √© criado mesclando o conhecimento de ambos os agentes
        treinados, resultando em um agente mais forte que combina as melhores
        estrat√©gias aprendidas por cada um.

        Note:
            Este m√©todo executa o script externo mesclar_modelos.py.
            O c√≥digo de retorno indica sucesso (0) ou falha (diferente de 0).
        """
        print("\n" + "="*50)
        print("üîÑ EXECUTANDO MESCLAGEM DOS MODELOS...")
        print("="*50 + "\n")
        
        # Executa o script Python de mesclagem
        codigo_retorno = os.system('py -m mesclar_modelos')
        
        if codigo_retorno == 0:
            print("\n‚úÖ Mesclagem conclu√≠da com sucesso!")
        else:
            print(f"\n‚ùå Erro ao executar mesclagem (c√≥digo: {codigo_retorno})")


# --- Bloco de Execu√ß√£o Principal ---
if __name__ == "__main__":
    # Op√ß√£o 1: Treinamento Padr√£o (3x3, 200.000 partidas)
    # Roda um treinamento completo e salva os modelos.
    ambiente_padrao = AmbienteJogoDaVelha(dimensao=3)
    agente_x_padrao = AgenteQLearning(jogador=1)
    agente_o_padrao = AgenteQLearning(jogador=2)
    
    treinador_padrao = Treinador(agente_x_padrao, agente_o_padrao, ambiente_padrao)
    treinador_padrao.treinar(numero_de_partidas=200000, intervalo_log=500, intervalo_checkpoint=40000)
    
    treinador_padrao.avaliar_agentes()
    
    treinador_padrao.mesclar_agentes_treinados()
    
    # Op√ß√£o 2: Treinamento Customizado (ex: 4x4, 100.000 partidas)
    # Descomente as linhas abaixo para rodar um treino diferente.
    # print("\n--- INICIANDO TREINAMENTO CUSTOMIZADO 4x4 ---")
    # ambiente_4x4 = AmbienteJogoDaVelha(dimensao=4)
    # agente_x_4x4 = AgenteQLearning(jogador=1, taxa_decaimento_epsilon=0.9999)
    # agente_o_4x4 = AgenteQLearning(jogador=2, taxa_decaimento_epsilon=0.9999)
    #
    # treinador_4x4 = Treinador(agente_x_4x4, agente_o_4x4, ambiente_4x4)
    # treinador_4x4.treinar(numero_de_partidas=100000, intervalo_log=10000)
