"""
M√≥dulo: üß† agente.py
Projeto: üìò AI Game Learning

Este m√≥dulo implementa um Agente de Aprendizado por Refor√ßo que utiliza o algoritmo
Q-Learning para aprender a jogar Jogo da Velha de forma aut√¥noma.

O agente √© projetado para ser compat√≠vel com dois modos de opera√ß√£o:
    - Treinamento em massa: utilizado pelo m√≥dulo treinador.py para treinar o agente
      atrav√©s de milhares de partidas em modo self-play.
    - Aprendizado interativo: utilizado pelo m√≥dulo jogar.py para permitir que o
      agente aprenda enquanto joga contra um humano.

O algoritmo Q-Learning funciona atrav√©s de uma tabela Q que armazena o valor esperado
de cada a√ß√£o poss√≠vel em cada estado do jogo. Com o tempo, o agente aprende quais
a√ß√µes levam a melhores resultados e ajusta sua estrat√©gia automaticamente.
"""

import random
import pickle
from typing import List, Tuple, Dict
from pathlib import Path


class AgenteQLearning:
    """
    Agente de Aprendizado por Refor√ßo que utiliza Q-Learning para jogar Jogo da Velha.

    Este agente aprende atrav√©s da explora√ß√£o (tentando a√ß√µes aleat√≥rias) e explora√ß√£o
    (usando o conhecimento adquirido). A estrat√©gia Epsilon-Greedy equilibra esses dois
    comportamentos, permitindo que o agente descubra novas estrat√©gias enquanto tamb√©m
    aproveita o conhecimento j√° adquirido.

    Analogia did√°tica: Pense neste Agente como um jogador de Ragnarok Online que est√°
    aprendendo a melhor estrat√©gia para derrotar monstros. Inicialmente, ele tenta
    diferentes abordagens (explora√ß√£o), mas com o tempo, ele passa a usar as estrat√©gias
    que funcionaram melhor no passado (explora√ß√£o).

    Attributes:
        alpha (float): Taxa de aprendizado (0.0 a 1.0). Controla o quanto o agente
            atualiza seus valores Q a cada aprendizado. Valores maiores fazem o agente
            aprender mais r√°pido, mas podem torn√°-lo inst√°vel.
        gamma (float): Fator de desconto (0.0 a 1.0). Determina o quanto o agente
            valoriza recompensas futuras em rela√ß√£o √†s imediatas. 1.0 significa que
            recompensas futuras s√£o t√£o importantes quanto as atuais.
        epsilon (float): Taxa de explora√ß√£o (0.0 a 1.0). Probabilidade de escolher
            uma a√ß√£o aleat√≥ria ao inv√©s da melhor a√ß√£o conhecida. Inicia em 1.0
            (100% explora√ß√£o) e decai com o tempo.
        epsilon_minimo (float): Valor m√≠nimo que epsilon pode atingir. Garante que
            o agente sempre mantenha um m√≠nimo de explora√ß√£o, mesmo ap√≥s muito treino.
        taxa_decaimento_epsilon (float): Taxa pela qual epsilon √© multiplicado a
            cada partida. Valores pr√≥ximos de 1.0 fazem o epsilon decair lentamente.
        jogador (int): Identificador do jogador (1 para 'X', 2 para 'O').
        simbolo (str): S√≠mbolo visual do jogador ('X' ou 'O').
        tabela_q (Dict[Tuple, Dict[int, float]]): Tabela Q que armazena o valor
            esperado de cada a√ß√£o em cada estado. Estrutura: {estado: {acao: valor_q}}.
        partidas_treinadas (int): Contador total de partidas em que o agente participou.
        vitorias (int): N√∫mero de partidas vencidas pelo agente.
        derrotas (int): N√∫mero de partidas perdidas pelo agente.
        empates (int): N√∫mero de partidas que terminaram em empate.
        historico_partida (List[Tuple[Tuple, int]]): Lista de (estado, a√ß√£o) registradas
            durante a partida atual. Usado para aprendizado Monte Carlo no final da partida.

    Example:
        >>> agente = AgenteQLearning(alpha=0.5, gamma=1.0, jogador=1)
        >>> estado = ((0, 0, 0, 0, 0, 0, 0, 0, 0),)
        >>> acoes_validas = [0, 1, 2, 3, 4, 5, 6, 7, 8]
        >>> acao_escolhida = agente.escolher_acao(estado, acoes_validas)
        >>> print(f"Agente escolheu a a√ß√£o: {acao_escolhida}")
    """

    def __init__(self,
                 alpha: float = 0.5,
                 gamma: float = 1.0,
                 epsilon: float = 1.0,
                 epsilon_minimo: float = 0.001,
                 taxa_decaimento_epsilon: float = 0.99999,
                 jogador: int = 1
                ):
        """
        Inicializa uma nova inst√¢ncia do Agente Q-Learning.

        Args:
            alpha: Taxa de aprendizado. Padr√£o: 0.5. Valores t√≠picos: 0.1 a 0.9.
            gamma: Fator de desconto para recompensas futuras. Padr√£o: 1.0.
                Para Jogo da Velha, 1.0 √© apropriado pois todas as jogadas
                s√£o igualmente importantes.
            epsilon: Taxa inicial de explora√ß√£o. Padr√£o: 1.0 (100% explora√ß√£o).
                O agente come√ßa totalmente explorat√≥rio e se torna mais
                explorador com o tempo.
            epsilon_minimo: Valor m√≠nimo de epsilon ap√≥s decaimento. Padr√£o: 0.001.
                Garante que o agente sempre mantenha um m√≠nimo de explora√ß√£o.
            taxa_decaimento_epsilon: Taxa de decaimento de epsilon por partida.
                Padr√£o: 0.99999 (decai muito lentamente). Valores menores
                fazem o epsilon decair mais r√°pido.
            jogador: Identificador do jogador. Padr√£o: 1 (jogador 'X').
                Use 2 para o jogador 'O'.

        Note:
            Todos os atributos de estat√≠sticas (vitorias, derrotas, empates)
            s√£o inicializados em zero e atualizados durante o treinamento.
        """
        self.alpha = alpha
        self.gamma = gamma
        self.epsilon = epsilon
        self.epsilon_minimo = epsilon_minimo
        self.taxa_decaimento_epsilon = taxa_decaimento_epsilon
        self.jogador = jogador
        self.simbolo = 'X' if jogador == 1 else 'O'
        self.tabela_q: Dict[Tuple, Dict[int, float]] = {}
        
        # Atributos para o treinamento em massa (gerenciados pelo treinador.py)
        self.partidas_treinadas = 0
        self.vitorias = 0
        self.derrotas = 0
        self.empates = 0
        self.historico_partida: List[Tuple[Tuple, int]] = []

    def obter_valor_q(self, estado: Tuple, acao: int) -> float:
        """
        Obt√©m o valor Q (valor esperado) de uma a√ß√£o espec√≠fica em um estado.

        Se o estado ou a a√ß√£o n√£o existirem na tabela Q, eles s√£o criados
        automaticamente com valor inicial de 0.0. Isso permite que o agente
        aprenda sobre novos estados e a√ß√µes conforme os encontra.

        Args:
            estado: Tupla representando o estado atual do tabuleiro.
            acao: √çndice da a√ß√£o (posi√ß√£o no tabuleiro de 0 a 8).

        Returns:
            Valor Q da a√ß√£o no estado especificado. Retorna 0.0 se o estado
            ou a√ß√£o ainda n√£o foram explorados.

        Note:
            Este m√©todo √© idempotente: m√∫ltiplas chamadas com os mesmos
            par√¢metros retornam o mesmo valor, mas podem criar entradas
            na tabela Q se elas n√£o existirem.
        """
        if estado not in self.tabela_q:
            self.tabela_q[estado] = {}
        if acao not in self.tabela_q[estado]:
            self.tabela_q[estado][acao] = 0.0
        return self.tabela_q[estado][acao]

    def atualizar_valor_q(self, estado: Tuple, acao: int, recompensa: float, proximo_estado: Tuple, finalizado: bool):
        """
        Atualiza o valor Q usando Temporal Difference (TD) Learning.

        Este √© o m√©todo principal de atualiza√ß√£o do valor Q do agente. Ele implementa a
        f√≥rmula cl√°ssica do Q-Learning:

            Q(estado, acao) = Q(estado, acao) + alpha * (recompensa + gamma * max(Q(proximo_estado)) - Q(estado, acao))

        O aprendizado funciona da seguinte forma:
        1. O agente compara sua "opini√£o antiga" (valor Q atual) com o "valor real"
           (recompensa imediata + melhor valor futuro).
        2. A diferen√ßa entre esses valores √© a "surpresa" (erro de predi√ß√£o).
        3. O agente ajusta seu valor Q proporcionalmente √† surpresa e √† taxa de aprendizado.

        Este m√©todo √© usado tanto para aprendizado incremental (TD Learning) durante
        partidas interativas quanto internamente pelo m√©todo de aprendizado Monte Carlo.

        Args:
            estado: Estado do tabuleiro antes da a√ß√£o ser executada.
            acao: A√ß√£o que foi tomada no estado.
            recompensa: Recompensa imediata recebida ap√≥s executar a a√ß√£o.
                Valores t√≠picos: +1.0 (vit√≥ria), -1.0 (derrota), 0.0 (empate/continua).
            proximo_estado: Estado do tabuleiro ap√≥s a a√ß√£o ser executada.
            finalizado: Se True, indica que o jogo terminou ap√≥s esta a√ß√£o.
                Quando True, n√£o h√° valor futuro a considerar.

        Note:
            Este m√©todo modifica diretamente a tabela Q do agente. O aprendizado
            √© incremental e baseado na diferen√ßa temporal (TD error), permitindo
            que o agente aprenda mesmo sem esperar o fim da partida.
        """
        opiniao_antiga = self.obter_valor_q(estado, acao)
        
        # Se o jogo terminou, n√£o h√° valor futuro a considerar
        melhor_valor_futuro = 0.0 if finalizado else self._obter_melhor_valor_q_futuro(proximo_estado)
        
        # Calcula o valor real da jogada (recompensa imediata + valor futuro descontado)
        valor_real_da_jogada = recompensa + self.gamma * melhor_valor_futuro
        
        # Calcula a "surpresa" (erro de predi√ß√£o)
        surpresa = valor_real_da_jogada - opiniao_antiga
        
        # Atualiza o valor Q proporcionalmente √† surpresa
        novo_valor_q = opiniao_antiga + self.alpha * surpresa
        self.tabela_q[estado][acao] = novo_valor_q

    def _obter_melhor_valor_q_futuro(self, estado: Tuple) -> float:
        """
        Obt√©m o maior valor Q dispon√≠vel para um estado futuro espec√≠fico.

        Este m√©todo √© usado para calcular o valor futuro esperado em um estado,
        assumindo que o agente escolher√° sempre a melhor a√ß√£o conhecida. √â usado
        na f√≥rmula do Q-Learning para estimar o valor das recompensas futuras.

        Args:
            estado: Tupla representando o estado futuro do tabuleiro.

        Returns:
            O maior valor Q entre todas as a√ß√µes conhecidas para este estado.
            Retorna 0.0 se o estado n√£o existe na tabela Q ou n√£o tem a√ß√µes registradas.

        Note:
            Este √© um m√©todo privado (prefixo _) usado internamente pelo m√©todo
            atualizar_valor_q() para calcular valores futuros esperados na f√≥rmula
            do Q-Learning.
        """
        if estado not in self.tabela_q or not self.tabela_q[estado]:
            return 0.0
        return max(self.tabela_q[estado].values())

    def escolher_acao(self, estado: Tuple, acoes_validas: List[int], em_treinamento: bool = True) -> int:
        """
        Escolhe uma a√ß√£o usando a estrat√©gia Epsilon-Greedy.

        A estrat√©gia Epsilon-Greedy equilibra explora√ß√£o e explora√ß√£o:
        - Com probabilidade epsilon: escolhe uma a√ß√£o aleat√≥ria (explora√ß√£o)
        - Com probabilidade (1 - epsilon): escolhe a melhor a√ß√£o conhecida (explora√ß√£o)

        Quando em_treinamento=False, o agente sempre escolhe a melhor a√ß√£o,
        ignorando epsilon. Isso √© √∫til para avalia√ß√£o ou quando o agente j√°
        est√° suficientemente treinado.

        Args:
            estado: Tupla representando o estado atual do tabuleiro.
            acoes_validas: Lista de √≠ndices de a√ß√µes v√°lidas (posi√ß√µes vazias no tabuleiro).
            em_treinamento: Se False, sempre escolhe a melhor a√ß√£o conhecida,
                ignorando epsilon. Padr√£o: True.

        Returns:
            √çndice da a√ß√£o escolhida (0 a 8, representando posi√ß√µes no tabuleiro).

        Raises:
            ValueError: Se a lista de a√ß√µes v√°lidas estiver vazia.

        Example:
            >>> agente = AgenteQLearning(epsilon=0.1)
            >>> estado = ((1, 0, 0, 0, 2, 0, 0, 0, 1),)
            >>> acoes = [1, 2, 3, 5, 6, 7]
            >>> acao = agente.escolher_acao(estado, acoes, em_treinamento=True)
            >>> # 90% das vezes escolhe a melhor a√ß√£o, 10% escolhe aleat√≥ria
        """
        if not acoes_validas:
            raise ValueError("N√£o h√° a√ß√µes v√°lidas para escolher.")
        
        # Se n√£o est√° em treinamento, sempre escolhe a melhor a√ß√£o
        if not em_treinamento:
            return self._escolher_melhor_acao(estado, acoes_validas)
        
        # Estrat√©gia Epsilon-Greedy: explora√ß√£o vs explora√ß√£o
        if random.random() < self.epsilon:
            # Explora√ß√£o: escolhe uma a√ß√£o aleat√≥ria
            return random.choice(acoes_validas)
        else:
            # Explora√ß√£o: escolhe a melhor a√ß√£o conhecida
            return self._escolher_melhor_acao(estado, acoes_validas)

    def _escolher_melhor_acao(self, estado: Tuple, acoes_validas: List[int]) -> int:
        """
        Escolhe a a√ß√£o com o maior valor Q entre as a√ß√µes v√°lidas.

        Se m√∫ltiplas a√ß√µes tiverem o mesmo valor Q m√°ximo, uma delas √©
        escolhida aleatoriamente. Isso adiciona um elemento de aleatoriedade
        mesmo na explora√ß√£o, evitando que o agente fique preso em padr√µes
        determin√≠sticos.

        Args:
            estado: Tupla representando o estado atual do tabuleiro.
            acoes_validas: Lista de √≠ndices de a√ß√µes v√°lidas.

        Returns:
            √çndice da a√ß√£o com o maior valor Q. Se houver empate, retorna
            uma das a√ß√µes empatadas escolhida aleatoriamente.

        Note:
            Este √© um m√©todo privado (prefixo _) usado internamente pelo
            m√©todo escolher_acao(). Ele assume que acoes_validas n√£o est√° vazia.
        """
        # Calcula o valor Q de cada a√ß√£o v√°lida
        valores_q_das_acoes = {acao: self.obter_valor_q(estado, acao) for acao in acoes_validas}
        
        # Encontra o maior valor Q
        valor_maximo_q = max(valores_q_das_acoes.values())
        
        # Seleciona todas as a√ß√µes que t√™m o valor m√°ximo
        melhores_acoes = [acao for acao, valor in valores_q_das_acoes.items() if valor == valor_maximo_q]
        
        # Se houver empate, escolhe aleatoriamente entre as melhores
        return random.choice(melhores_acoes)

    # --- M√âTODOS PARA O TREINAMENTO EM MASSA (usados pelo treinador.py) ---

    def limpar_historico_partida(self):
        """
        Limpa o hist√≥rico de jogadas da partida atual.

        Este m√©todo deve ser chamado no in√≠cio de cada partida durante o
        treinamento em massa. Ele limpa o hist√≥rico de jogadas da partida
        anterior, permitindo que o agente comece a registrar novas jogadas
        para a nova partida.

        Note:
            Este m√©todo n√£o afeta a tabela Q nem as estat√≠sticas gerais do agente.
            Apenas limpa o hist√≥rico de curto prazo da partida atual, que ser√°
            usado para aprendizado Monte Carlo no final da partida.
        """
        self.historico_partida = []

    def adicionar_jogada_ao_historico(self, estado: Tuple, acao: int):
        """
        Adiciona uma jogada ao hist√≥rico da partida atual.

        Durante o treinamento em massa, todas as jogadas s√£o registradas
        no hist√≥rico para que o agente possa aprender com elas no final da
        partida usando o m√©todo Monte Carlo. O hist√≥rico armazena a sequ√™ncia
        de (estado, a√ß√£o) que levaram ao resultado final.

        Args:
            estado: Estado do tabuleiro no momento da jogada.
            acao: A√ß√£o (posi√ß√£o) escolhida pelo agente.

        Note:
            Este m√©todo deve ser chamado para cada jogada do agente durante
            uma partida. O hist√≥rico √© processado no final da partida pelo
            m√©todo processar_aprendizado_monte_carlo().
        """
        self.historico_partida.append((estado, acao))

    def processar_aprendizado_monte_carlo(self, recompensa_final: float):
        """
        Processa o aprendizado usando o m√©todo Monte Carlo baseado no resultado final.

        Este m√©todo implementa aprendizado Monte Carlo, onde o agente aprende
        com base no resultado final da partida. Ele percorre o hist√≥rico de
        jogadas de tr√°s para frente, atribuindo recompensas descontadas a cada
        jogada anterior usando o m√©todo atualizar_valor_q().

        O processo funciona assim:
        1. Atualiza as estat√≠sticas (vit√≥rias, derrotas, empates).
        2. Percorre o hist√≥rico de jogadas de tr√°s para frente.
        3. Para cada jogada, aplica a recompensa final descontada pelo fator gamma.
        4. A recompensa √© descontada exponencialmente: jogadas mais recentes
           recebem mais cr√©dito do que jogadas antigas.
        5. Reduz epsilon (taxa de explora√ß√£o) para o pr√≥ximo treinamento.

        Este m√©todo √© usado principalmente pelo m√≥dulo treinador.py durante
        o treinamento em massa (self-play).

        Args:
            recompensa_final: Recompensa recebida no final da partida.
                Valores t√≠picos: +1.0 (vit√≥ria), -1.0 (derrota), 0.0 (empate).

        Note:
            Este m√©todo modifica a tabela Q e as estat√≠sticas do agente.
            Ap√≥s chamar este m√©todo, o hist√≥rico da partida ainda est√° dispon√≠vel,
            mas ser√° limpo na pr√≥xima chamada de limpar_historico_partida().
        """
        # Atualiza contadores de estat√≠sticas
        self.partidas_treinadas += 1
        if recompensa_final > 0:
            self.vitorias += 1
        elif recompensa_final < 0:
            self.derrotas += 1
        else:
            self.empates += 1

        # Aprendizado Monte Carlo: percorre o hist√≥rico de tr√°s para frente
        # A recompensa √© descontada exponencialmente (jogadas recentes valem mais)
        recompensa_atual = recompensa_final
        for estado, acao in reversed(self.historico_partida):
            # Reutiliza o m√©todo atualizar_valor_q() para manter a l√≥gica centralizada
            # finalizado=True porque estamos processando uma partida j√° finalizada
            self.atualizar_valor_q(estado, acao, recompensa_atual, estado, finalizado=True)
            # Desconta a recompensa para a pr√≥xima jogada (mais antiga)
            recompensa_atual *= self.gamma
        
        # Reduz a taxa de explora√ß√£o ap√≥s cada partida
        self.reduzir_epsilon()

    def reduzir_epsilon(self):
        """
        Reduz a taxa de explora√ß√£o (epsilon) do agente.

        Este m√©todo implementa o decaimento de epsilon, fazendo com que o
        agente se torne gradualmente menos explorat√≥rio e mais explorador
        conforme ganha experi√™ncia. O epsilon nunca cai abaixo de
        epsilon_minimo, garantindo que o agente sempre mantenha um m√≠nimo
        de explora√ß√£o.

        A f√≥rmula aplicada √©:
            epsilon = max(epsilon_minimo, epsilon * taxa_decaimento_epsilon)

        Note:
            Este m√©todo √© chamado automaticamente ap√≥s cada partida durante
            o treinamento em massa. Para aprendizado interativo, o decaimento
            pode ser controlado manualmente.
        """
        self.epsilon = max(self.epsilon_minimo, self.epsilon * self.taxa_decaimento_epsilon)

    def salvar_memoria(self, caminho: str):
        """
        Salva a tabela Q do agente em um arquivo usando pickle.

        A tabela Q cont√©m todo o conhecimento adquirido pelo agente durante
        o treinamento. Salvar a mem√≥ria permite que o agente retome seu
        aprendizado de onde parou, sem precisar treinar novamente do zero.

        O arquivo √© salvo em formato bin√°rio (pickle), o que permite preservar
        a estrutura completa da tabela Q (dicion√°rios aninhados).

        Args:
            caminho: Caminho do arquivo onde a tabela Q ser√° salva.
                Se o diret√≥rio n√£o existir, ele ser√° criado automaticamente.
                Exemplo: "modelos/agente_x.pkl"

        Note:
            Este m√©todo n√£o imprime mensagens de confirma√ß√£o, permitindo que
            o salvamento seja silencioso quando necess√°rio. Se voc√™ precisar
            de feedback, verifique o retorno ou trate exce√ß√µes.

        Example:
            >>> agente = AgenteQLearning()
            >>> # ... treinar o agente ...
            >>> agente.salvar_memoria("modelos/meu_agente.pkl")
        """
        caminho_arquivo = Path(caminho)
        caminho_arquivo.parent.mkdir(parents=True, exist_ok=True)
        with open(caminho_arquivo, 'wb') as arquivo:
            pickle.dump(self.tabela_q, arquivo)

    @classmethod
    def carregar(cls, caminho: str, **kwargs) -> 'AgenteQLearning':
        """
        Cria uma inst√¢ncia do agente e carrega sua tabela Q de um arquivo.

        Este m√©todo de classe permite criar um agente j√° treinado a partir de
        um arquivo salvo anteriormente. Se o arquivo n√£o existir, o agente ser√°
        criado com uma tabela Q vazia (sem conhecimento pr√©vio).

        Args:
            caminho: Caminho do arquivo contendo a tabela Q salva.
                Exemplo: "modelos/agente_x.pkl"
            **kwargs: Argumentos adicionais passados para o construtor do agente.
                Permite personalizar par√¢metros como alpha, gamma, epsilon, etc.
                mesmo ao carregar um modelo existente.

        Returns:
            Inst√¢ncia de AgenteQLearning com a tabela Q carregada (se o arquivo
            existir) ou vazia (se o arquivo n√£o existir).

        Note:
            Este m√©todo imprime mensagens informativas sobre o carregamento.
            Se o arquivo n√£o existir, o agente come√ßar√° do zero, mas manter√°
            os par√¢metros especificados em **kwargs.

        Example:
            >>> # Carregar agente com par√¢metros padr√£o
            >>> agente = AgenteQLearning.carregar("modelos/agente_x.pkl")
            >>> 
            >>> # Carregar agente com par√¢metros personalizados
            >>> agente = AgenteQLearning.carregar(
            ...     "modelos/agente_x.pkl",
            ...     alpha=0.3,
            ...     epsilon=0.01
            ... )
        """
        agente = cls(**kwargs)
        caminho_arquivo = Path(caminho)
        if caminho_arquivo.exists():
            with open(caminho_arquivo, 'rb') as arquivo:
                agente.tabela_q = pickle.load(arquivo)
            print(f"‚úÖ Mem√≥ria do Agente ({agente.simbolo}) carregada de: {caminho_arquivo}")
        else:
            print(f"‚ö†Ô∏è  Aviso: Nenhum arquivo de mem√≥ria encontrado em {caminho}. "
                  f"O Agente ({agente.simbolo}) come√ßar√° do zero.")
        return agente

    def imprimir_estatisticas(self):
        """
        Imprime as estat√≠sticas de treinamento do agente de forma formatada.

        Exibe informa√ß√µes sobre o desempenho do agente, incluindo:
        - N√∫mero total de partidas treinadas
        - Quantidade de estados √∫nicos conhecidos (tamanho da tabela Q)
        - Taxa de explora√ß√£o atual (epsilon)
        - Estat√≠sticas de desempenho: vit√≥rias, empates e derrotas com percentuais

        A sa√≠da √© formatada de forma leg√≠vel, com separadores visuais e
        n√∫meros formatados com separadores de milhar.

        Note:
            Este m√©todo n√£o retorna nada, apenas imprime no console.
            Se nenhuma partida foi treinada, todas as taxas ser√£o 0.0%.
        """
        total_jogos = self.vitorias + self.derrotas + self.empates
        
        # Calcula as taxas de desempenho
        if total_jogos == 0:
            taxa_vitoria, taxa_empate, taxa_derrota = 0.0, 0.0, 0.0
        else:
            taxa_vitoria = self.vitorias / total_jogos
            taxa_empate = self.empates / total_jogos
            taxa_derrota = self.derrotas / total_jogos

        # Imprime as estat√≠sticas formatadas
        print(f"\n{'='*50}")
        print(f"üìä ESTAT√çSTICAS DO AGENTE ({self.simbolo})")
        print(f"{'='*50}")
        print(f"Partidas treinadas:   {self.partidas_treinadas:,}")
        print(f"Estados conhecidos:   {len(self.tabela_q):,}")
        print(f"Curiosidade (Epsilon):{self.epsilon:.4f}")
        print(f"\n--- Desempenho ---")
        print(f"Vit√≥rias:   {self.vitorias:>6} ({taxa_vitoria*100:>5.1f}%)")
        print(f"Empates:    {self.empates:>6} ({taxa_empate*100:>5.1f}%)")
        print(f"Derrotas:   {self.derrotas:>6} ({taxa_derrota*100:>5.1f}%)")
        print(f"{'='*50}\n")