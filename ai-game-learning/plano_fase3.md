# Implementa√ß√£o do Agente Q-Learning para Labirinto (Fase 3)

## üéØ Objetivo

Implementar um agente de Aprendizado por Refor√ßo (Q-Learning) capaz de resolver o ambiente do Labirinto (`fase-3`). O agente deve aprender a navegar do ponto inicial at√© a sa√≠da, evitando paredes e otimizando o caminho.

## üìù Contexto

J√° temos:

- **Ambiente (`ambiente.py`):** Define o labirinto, estados (posi√ß√µes), a√ß√µes (movimentos) e recompensas.
- **Jogo Manual (`jogar.py`):** Permite jogar manualmente e visualizar o labirinto.

Precisamos criar:

- **Agente (`agente.py`):** O "c√©rebro" que aprender√° a jogar.
- **Treinador (`treinador.py`):** Script para rodar m√∫ltiplos epis√≥dios de treinamento (sem interface gr√°fica) para o agente aprender.

## üõ†Ô∏è Altera√ß√µes Propostas

### 1. Criar `fase-3/agente.py`

Baseado no `agente.py` do Jogo da Velha, mas adaptado:

- **Estado:** Em vez de uma tupla de 9 n√∫meros (tabuleiro), o estado ser√° uma tupla `(linha, coluna)` representando a posi√ß√£o do agente.
- **A√ß√µes:** Em vez de √≠ndices 0-8, as a√ß√µes ser√£o strings: `['cima', 'baixo', 'esquerda', 'direita']` (ou √≠ndices mapeados para essas strings).
- **Q-Table:** Dicion√°rio mapeando `(linha, coluna) -> {acao: valor_q}`.

### 2. Criar `fase-3/treinador.py`

Script para gerenciar o ciclo de vida do treinamento:

- Instanciar Ambiente e Agente.
- Loop de Epis√≥dios (ex: 1000 partidas).
- Loop de Passos (dentro de cada epis√≥dio):
  - Agente escolhe a√ß√£o.
  - Ambiente executa.
  - Agente recebe recompensa e atualiza Q-Table.
- Exibir estat√≠sticas de evolu√ß√£o (recompensa m√©dia, passos at√© a sa√≠da).

### 3. Atualizar `fase-3/jogar.py` (Opcional/Futuro)

- Adicionar modo para assistir a IA jogando (carregar modelo treinado).

## ‚ö†Ô∏è Pontos de Aten√ß√£o

- **Explora√ß√£o vs Explora√ß√£o:** Manter epsilon-greedy.
- **Recompensas:** O ambiente j√° penaliza passos (-0.1) e premia a sa√≠da. Isso deve ser suficiente para buscar o caminho mais curto.
- **Loop Infinito:** No in√≠cio, o agente pode ficar andando em c√≠rculos. Precisamos limitar o n√∫mero m√°ximo de passos por epis√≥dio.

## üìã Plano de Execu√ß√£o

1.  [ ] Criar `fase-3/agente.py`
2.  [ ] Criar `fase-3/treinador.py`
3.  [ ] Rodar treinamento e validar aprendizado.
